{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "96b50ed1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn, Tensor\n",
    "from torch.autograd import Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0149bd2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mlp_fwd(x, W1, W2, W3):\n",
    "    return (torch.nn.functional.silu(x @ W2.T) * (x @ W1.T)) @ W3.T\n",
    "\n",
    "class LlamaMLP(nn.Module):\n",
    "    def __init__(self, hidden_size:int = 64, intermediate_size:int = 256):\n",
    "        super().__init__()\n",
    "        self.W1 = nn.Linear(hidden_size, intermediate_size, bias=False)\n",
    "        self.W2 = nn.Linear(hidden_size, intermediate_size, bias=False)\n",
    "        self.W3 = nn.Linear(intermediate_size, hidden_size, bias=False)\n",
    "        self.act = nn.SiLU()\n",
    "\n",
    "    def forward(self, x:Tensor) -> Tensor:\n",
    "        return mlp_fwd(x, self.W1.weight, self.W2.weight, self.W3.weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "304f8d88",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LlamaMLPFunction(Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, x, W1, W2, W3):\n",
    "        a = x @ W1.T\n",
    "        b = x @ W2.T\n",
    "        sigma = 1 / (1 + torch.exp(-b))\n",
    "        c = b * sigma\n",
    "        d = a * c\n",
    "        e = d @ W3.T\n",
    "        # ctx.save_for_backward(x, W1, W2, W3, a, b, c, d, sigma)\n",
    "        ctx.save_for_backward(x, W1, W2, W3)\n",
    "        return e\n",
    "    \n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        x, W1, W2, W3 = ctx.saved_tensors\n",
    "        B, S, D = x.shape\n",
    "        D_in = W3.shape[1]\n",
    "\n",
    "        a = x @ W1.T\n",
    "        b = x @ W2.T\n",
    "        sigma = 1 / (1 + torch.exp(-b))\n",
    "        c = b * sigma\n",
    "        d = a * c\n",
    "        \n",
    "        # dW3\n",
    "        dW3 =  grad_output.permute(B, D, S) @ d\n",
    "\n",
    "        # dW2\n",
    "        act_prime = sigma * (1 + b * (1 - sigma))\n",
    "        dL_db = (((grad_output @ W3) * a) * act_prime)\n",
    "        dW2 = dL_db.permute(B, D_in, S) @ x \n",
    "        \n",
    "        # dW1\n",
    "        dL_da = ((grad_output @ W3) * c)\n",
    "        dW1 = dL_da.permute(B, D_in, S) @ x \n",
    "\n",
    "        # dx\n",
    "        dx = dL_da @ W1 + dL_db @ W2\n",
    "\n",
    "        return dx, dW1, dW2, dW3\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "e16f3389",
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp = LlamaMLP()\n",
    "\n",
    "x = torch.rand(256, 64)\n",
    "assert torch.allclose(\n",
    "    mlp(x), \n",
    "    LlamaMLPFunction.apply(x, mlp.W1.weight, mlp.W2.weight, mlp.W3.weight), \n",
    "    atol=1e-7\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "789e9f9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient 0: max diff = 5.96e-08\n",
      "Gradient 1: max diff = 5.96e-07\n",
      "Gradient 2: max diff = 7.15e-07\n",
      "Gradient 3: max diff = 1.43e-06\n"
     ]
    }
   ],
   "source": [
    "### Check backward pass\n",
    "x = torch.rand(256, 64, requires_grad=True)\n",
    "\n",
    "# Forward pass\n",
    "out_a = LlamaMLPFunction.apply(x, mlp.W1.weight, mlp.W2.weight, mlp.W3.weight)\n",
    "out_b = mlp_fwd(x, mlp.W1.weight, mlp.W2.weight, mlp.W3.weight)\n",
    "\n",
    "grad_output = torch.randn_like(out_a)\n",
    "\n",
    "# Backward pass A\n",
    "grads_a = torch.autograd.grad(out_a, (x, mlp.W1.weight, mlp.W2.weight, mlp.W3.weight), grad_outputs=grad_output, retain_graph=True)\n",
    "\n",
    "# Backward pass B\n",
    "grads_b = torch.autograd.grad(out_b, (x, mlp.W1.weight, mlp.W2.weight, mlp.W3.weight), grad_outputs=grad_output, retain_graph=True)\n",
    "\n",
    "for i, (ga, gb) in enumerate(zip(grads_a, grads_b)):\n",
    "    max_diff = (ga - gb).abs().max().item()\n",
    "    print(f\"Gradient {i}: max diff = {max_diff:3.3}\")\n",
    "    assert torch.allclose(ga, gb, atol=1e-6), f\"Mismatch in grad {i}\"\n",
    "    \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
