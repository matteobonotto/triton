{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "96b50ed1",
   "metadata": {
    "executionInfo": {
     "elapsed": 3863,
     "status": "ok",
     "timestamp": 1757603837508,
     "user": {
      "displayName": "Matteo Bonotto",
      "userId": "01035356913648804886"
     },
     "user_tz": -120
    },
    "id": "96b50ed1"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn, Tensor\n",
    "from torch.autograd import Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0149bd2d",
   "metadata": {
    "executionInfo": {
     "elapsed": 1,
     "status": "ok",
     "timestamp": 1757603837515,
     "user": {
      "displayName": "Matteo Bonotto",
      "userId": "01035356913648804886"
     },
     "user_tz": -120
    },
    "id": "0149bd2d"
   },
   "outputs": [],
   "source": [
    "def mlp_fwd(x, W1, W2, W3):\n",
    "    return (torch.nn.functional.silu(x @ W2.T) * (x @ W1.T)) @ W3.T\n",
    "\n",
    "class LlamaMLP(nn.Module):\n",
    "    def __init__(self, hidden_size:int = 64, intermediate_size:int = 256):\n",
    "        super().__init__()\n",
    "        self.W1 = nn.Linear(hidden_size, intermediate_size, bias=False)\n",
    "        self.W2 = nn.Linear(hidden_size, intermediate_size, bias=False)\n",
    "        self.W3 = nn.Linear(intermediate_size, hidden_size, bias=False)\n",
    "        self.act = nn.SiLU()\n",
    "\n",
    "    def forward(self, x:Tensor) -> Tensor:\n",
    "        return mlp_fwd(x, self.W1.weight, self.W2.weight, self.W3.weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "aea0bba3",
   "metadata": {
    "executionInfo": {
     "elapsed": 68,
     "status": "ok",
     "timestamp": 1757603857846,
     "user": {
      "displayName": "Matteo Bonotto",
      "userId": "01035356913648804886"
     },
     "user_tz": -120
    },
    "id": "aea0bba3"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "from typing import Dict, Any\n",
    "import psutil\n",
    "from time import time\n",
    "\n",
    "class CUDAMemTracker:\n",
    "    def __init__(self, device=None):\n",
    "        self.device = torch.device(device or \"cuda\")\n",
    "\n",
    "    def __enter__(self):\n",
    "        torch.cuda.synchronize(self.device)\n",
    "        torch.cuda.reset_peak_memory_stats(self.device)\n",
    "        self.start_alloc = torch.cuda.memory_allocated(self.device)\n",
    "        self.start_reserved = torch.cuda.memory_reserved(self.device)\n",
    "        self.start_rss = psutil.Process(os.getpid()).memory_info().rss if psutil else None\n",
    "        return self\n",
    "\n",
    "    def __exit__(self, exc_type, exc, tb):\n",
    "        torch.cuda.synchronize(self.device)\n",
    "        self.end_alloc = torch.cuda.memory_allocated(self.device)\n",
    "        self.end_reserved = torch.cuda.memory_reserved(self.device)\n",
    "        self.peak_alloc = torch.cuda.max_memory_allocated(self.device)\n",
    "        self.peak_reserved = torch.cuda.max_memory_reserved(self.device)\n",
    "\n",
    "    def report(self, unit=1024**2):\n",
    "        to_mb = lambda b: None if b is None else b / unit\n",
    "        return {\n",
    "            \"alloc_delta_MB\": to_mb(self.end_alloc - self.start_alloc),\n",
    "            \"reserved_delta_MB\": to_mb(self.end_reserved - self.start_reserved),\n",
    "            \"peak_alloc_MB\": to_mb(self.peak_alloc),\n",
    "            \"peak_reserved_MB\": to_mb(self.peak_reserved),\n",
    "            \"cpu_rss_MB\": to_mb(self.start_rss) if self.start_rss is not None else None,\n",
    "        }\n",
    "\n",
    "\n",
    "\n",
    "def profile_torch_module_forward(module: torch.nn.Module, inputs: Dict[str, Any]):\n",
    "    t0 = time()\n",
    "    with CUDAMemTracker() as t:\n",
    "        module(**inputs)\n",
    "    t1 = time() - t0\n",
    "\n",
    "    report = t.report()\n",
    "    print(f'Elapsed time:       {t1:3.3}s')\n",
    "    print(f'Peak allocated mem: {report['peak_alloc_MB']:3.3}MB')\n",
    "    print(f'Peak reserved mem:  {report['peak_alloc_MB']:3.3}MB')\n",
    "\n",
    "\n",
    "def profile_torch_module_backward(module: torch.nn.Module, inputs: Dict[str, Any]):\n",
    "    t0 = time()\n",
    "    with CUDAMemTracker() as t:\n",
    "        out = module(**inputs)\n",
    "        loss = out.sum()\n",
    "        loss.backward()\n",
    "    t1 = time() - t0\n",
    "\n",
    "    report = t.report()\n",
    "    print(f'Elapsed time:       {t1:3.3}s')\n",
    "    print(f'Peak allocated mem: {report['peak_alloc_MB']:3.3}MB')\n",
    "    print(f'Peak reserved mem:  {report['peak_alloc_MB']:3.3}MB')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "304f8d88",
   "metadata": {
    "executionInfo": {
     "elapsed": 22,
     "status": "ok",
     "timestamp": 1757603862505,
     "user": {
      "displayName": "Matteo Bonotto",
      "userId": "01035356913648804886"
     },
     "user_tz": -120
    },
    "id": "304f8d88"
   },
   "outputs": [],
   "source": [
    "class LlamaMLPFunction(Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, x, W1, W2, W3):\n",
    "        a = x @ W1.T\n",
    "        b = x @ W2.T\n",
    "        sigma = 1 / (1 + torch.exp(-b))\n",
    "        c = b * sigma\n",
    "        d = a * c\n",
    "        e = d @ W3.T\n",
    "        # ctx.save_for_backward(x, W1, W2, W3, a, b, c, d, sigma)\n",
    "        ctx.save_for_backward(x, W1, W2, W3)\n",
    "        return e\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        x, W1, W2, W3 = ctx.saved_tensors\n",
    "        B, S, D = x.shape\n",
    "        D_in = W3.shape[1]\n",
    "\n",
    "        a = x @ W1.T\n",
    "        b = x @ W2.T\n",
    "        sigma = 1 / (1 + torch.exp(-b))\n",
    "        c = b * sigma\n",
    "        d = a * c\n",
    "\n",
    "        # dW3\n",
    "        dW3 =  grad_output.permute(0, 2, 1) @ d\n",
    "\n",
    "        # dW2\n",
    "        act_prime = sigma * (1 + b * (1 - sigma))\n",
    "        dL_db = (((grad_output @ W3) * a) * act_prime)\n",
    "        dW2 = dL_db.permute(B, D_i) @ x\n",
    "\n",
    "        # dW1\n",
    "        dL_da = ((grad_output @ W3) * c)\n",
    "        dW1 = dL_da.permute(B, D_in, S) @ x\n",
    "\n",
    "        # dx\n",
    "        dx = dL_da @ W1 + dL_db @ W2\n",
    "\n",
    "        return dx, dW1, dW2, dW3\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e16f3389",
   "metadata": {
    "executionInfo": {
     "elapsed": 63,
     "status": "ok",
     "timestamp": 1757603966211,
     "user": {
      "displayName": "Matteo Bonotto",
      "userId": "01035356913648804886"
     },
     "user_tz": -120
    },
    "id": "e16f3389"
   },
   "outputs": [],
   "source": [
    "B, S, D = 4, 1000, 128\n",
    "x = torch.rand(B, S, D, requires_grad=True)\n",
    "\n",
    "mlp = LlamaMLP(hidden_size=D)\n",
    "\n",
    "assert torch.allclose(\n",
    "    mlp(x),\n",
    "    LlamaMLPFunction.apply(x, mlp.W1.weight, mlp.W2.weight, mlp.W3.weight),\n",
    "    atol=1e-7\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "789e9f9f",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 365
    },
    "executionInfo": {
     "elapsed": 200,
     "status": "error",
     "timestamp": 1757603971204,
     "user": {
      "displayName": "Matteo Bonotto",
      "userId": "01035356913648804886"
     },
     "user_tz": -120
    },
    "id": "789e9f9f",
    "outputId": "20dafc8e-2ac4-4659-87fe-7eb550c104cf"
   },
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "Dimension out of range (expected to be in range of [-3, 2], but got 4)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipython-input-2594435688.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;31m# Backward pass A\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0mgrads_a\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_a\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmlp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mW1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmlp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mW2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmlp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mW3\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_outputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgrad_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;31m# Backward pass B\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mgrad\u001b[0;34m(outputs, inputs, grad_outputs, retain_graph, create_graph, only_inputs, allow_unused, is_grads_batched, materialize_grads)\u001b[0m\n\u001b[1;32m    501\u001b[0m         )\n\u001b[1;32m    502\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 503\u001b[0;31m         result = _engine_run_backward(\n\u001b[0m\u001b[1;32m    504\u001b[0m             \u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    505\u001b[0m             \u001b[0mgrad_outputs_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/autograd/graph.py\u001b[0m in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    827\u001b[0m         \u001b[0munregister_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_register_logging_hooks_on_whole_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    828\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 829\u001b[0;31m         return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    830\u001b[0m             \u001b[0mt_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    831\u001b[0m         )  # Calls into the C++ engine to run the backward pass\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/autograd/function.py\u001b[0m in \u001b[0;36mapply\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    309\u001b[0m             )\n\u001b[1;32m    310\u001b[0m         \u001b[0muser_fn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvjp_fn\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mvjp_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mFunction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvjp\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mbackward_fn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 311\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0muser_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    312\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    313\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mapply_jvp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipython-input-1742557059.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(ctx, grad_output)\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0;31m# dW3\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m         \u001b[0mdW3\u001b[0m \u001b[0;34m=\u001b[0m  \u001b[0mgrad_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpermute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mB\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mD\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mS\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m@\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m         \u001b[0;31m# dW2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: Dimension out of range (expected to be in range of [-3, 2], but got 4)"
     ]
    }
   ],
   "source": [
    "### Check backward pass\n",
    "\n",
    "\n",
    "# Forward pass\n",
    "out_a = LlamaMLPFunction.apply(x, mlp.W1.weight, mlp.W2.weight, mlp.W3.weight)\n",
    "out_b = mlp_fwd(x, mlp.W1.weight, mlp.W2.weight, mlp.W3.weight)\n",
    "\n",
    "grad_output = torch.randn_like(out_a)\n",
    "\n",
    "# Backward pass A\n",
    "grads_a = torch.autograd.grad(out_a, (x, mlp.W1.weight, mlp.W2.weight, mlp.W3.weight), grad_outputs=grad_output, retain_graph=True)\n",
    "\n",
    "# Backward pass B\n",
    "grads_b = torch.autograd.grad(out_b, (x, mlp.W1.weight, mlp.W2.weight, mlp.W3.weight), grad_outputs=grad_output, retain_graph=True)\n",
    "\n",
    "for i, (ga, gb) in enumerate(zip(grads_a, grads_b)):\n",
    "    max_diff = (ga - gb).abs().max().item()\n",
    "    print(f\"Gradient {i}: max diff = {max_diff:3.3}\")\n",
    "    assert torch.allclose(ga, gb, atol=1e-6), f\"Mismatch in grad {i}\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "R6rYQ7swWrow",
   "metadata": {
    "id": "R6rYQ7swWrow"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
