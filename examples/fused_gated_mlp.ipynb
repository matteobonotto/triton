{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "96b50ed1",
   "metadata": {
    "executionInfo": {
     "elapsed": 3863,
     "status": "ok",
     "timestamp": 1757603837508,
     "user": {
      "displayName": "Matteo Bonotto",
      "userId": "01035356913648804886"
     },
     "user_tz": -120
    },
    "id": "96b50ed1"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn, Tensor\n",
    "from torch.autograd import Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0149bd2d",
   "metadata": {
    "executionInfo": {
     "elapsed": 1,
     "status": "ok",
     "timestamp": 1757603837515,
     "user": {
      "displayName": "Matteo Bonotto",
      "userId": "01035356913648804886"
     },
     "user_tz": -120
    },
    "id": "0149bd2d"
   },
   "outputs": [],
   "source": [
    "def mlp_fwd(x, W1, W2, W3):\n",
    "    return (torch.nn.functional.silu(x @ W2.T) * (x @ W1.T)) @ W3.T\n",
    "\n",
    "class LlamaMLP(nn.Module):\n",
    "    def __init__(self, hidden_size:int = 64, intermediate_size:int = 256):\n",
    "        super().__init__()\n",
    "        self.W1 = nn.Linear(hidden_size, intermediate_size, bias=False)\n",
    "        self.W2 = nn.Linear(hidden_size, intermediate_size, bias=False)\n",
    "        self.W3 = nn.Linear(intermediate_size, hidden_size, bias=False)\n",
    "        self.act = nn.SiLU()\n",
    "\n",
    "    def forward(self, x:Tensor) -> Tensor:\n",
    "        return mlp_fwd(x, self.W1.weight, self.W2.weight, self.W3.weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "aea0bba3",
   "metadata": {
    "executionInfo": {
     "elapsed": 68,
     "status": "ok",
     "timestamp": 1757603857846,
     "user": {
      "displayName": "Matteo Bonotto",
      "userId": "01035356913648804886"
     },
     "user_tz": -120
    },
    "id": "aea0bba3"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "from typing import Dict, Any\n",
    "import psutil\n",
    "from time import time\n",
    "\n",
    "class CUDAMemTracker:\n",
    "    def __init__(self, device=None):\n",
    "        self.device = torch.device(device or \"cuda\")\n",
    "\n",
    "    def __enter__(self):\n",
    "        torch.cuda.synchronize(self.device)\n",
    "        torch.cuda.reset_peak_memory_stats(self.device)\n",
    "        self.start_alloc = torch.cuda.memory_allocated(self.device)\n",
    "        self.start_reserved = torch.cuda.memory_reserved(self.device)\n",
    "        self.start_rss = psutil.Process(os.getpid()).memory_info().rss if psutil else None\n",
    "        return self\n",
    "\n",
    "    def __exit__(self, exc_type, exc, tb):\n",
    "        torch.cuda.synchronize(self.device)\n",
    "        self.end_alloc = torch.cuda.memory_allocated(self.device)\n",
    "        self.end_reserved = torch.cuda.memory_reserved(self.device)\n",
    "        self.peak_alloc = torch.cuda.max_memory_allocated(self.device)\n",
    "        self.peak_reserved = torch.cuda.max_memory_reserved(self.device)\n",
    "\n",
    "    def report(self, unit=1024**2):\n",
    "        to_mb = lambda b: None if b is None else b / unit\n",
    "        return {\n",
    "            \"alloc_delta_MB\": to_mb(self.end_alloc - self.start_alloc),\n",
    "            \"reserved_delta_MB\": to_mb(self.end_reserved - self.start_reserved),\n",
    "            \"peak_alloc_MB\": to_mb(self.peak_alloc),\n",
    "            \"peak_reserved_MB\": to_mb(self.peak_reserved),\n",
    "            \"cpu_rss_MB\": to_mb(self.start_rss) if self.start_rss is not None else None,\n",
    "        }\n",
    "\n",
    "\n",
    "\n",
    "def profile_torch_module_forward(module: torch.nn.Module, inputs: Dict[str, Any]):\n",
    "    t0 = time()\n",
    "    with CUDAMemTracker() as t:\n",
    "        module(**inputs)\n",
    "    t1 = time() - t0\n",
    "\n",
    "    report = t.report()\n",
    "    print(f'Elapsed time:       {t1:3.3}s')\n",
    "    print(f'Peak allocated mem: {report['peak_alloc_MB']:3.3}MB')\n",
    "    print(f'Peak reserved mem:  {report['peak_alloc_MB']:3.3}MB')\n",
    "\n",
    "\n",
    "def profile_torch_module_backward(module: torch.nn.Module, inputs: Dict[str, Any]):\n",
    "    t0 = time()\n",
    "    with CUDAMemTracker() as t:\n",
    "        out = module(**inputs)\n",
    "        loss = out.sum()\n",
    "        loss.backward()\n",
    "    t1 = time() - t0\n",
    "\n",
    "    report = t.report()\n",
    "    print(f'Elapsed time:       {t1:3.3}s')\n",
    "    print(f'Peak allocated mem: {report['peak_alloc_MB']:3.3}MB')\n",
    "    print(f'Peak reserved mem:  {report['peak_alloc_MB']:3.3}MB')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "304f8d88",
   "metadata": {
    "executionInfo": {
     "elapsed": 22,
     "status": "ok",
     "timestamp": 1757603862505,
     "user": {
      "displayName": "Matteo Bonotto",
      "userId": "01035356913648804886"
     },
     "user_tz": -120
    },
    "id": "304f8d88"
   },
   "outputs": [],
   "source": [
    "class LlamaMLPFunction(Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, x, W1, W2, W3):\n",
    "        a = x @ W1.T\n",
    "        b = x @ W2.T\n",
    "        sigma = 1 / (1 + torch.exp(-b))\n",
    "        c = b * sigma\n",
    "        d = a * c\n",
    "        e = d @ W3.T\n",
    "        # ctx.save_for_backward(x, W1, W2, W3, a, b, c, d, sigma)\n",
    "        ctx.save_for_backward(x, W1, W2, W3)\n",
    "        return e\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        x, W1, W2, W3 = ctx.saved_tensors\n",
    "        B, S, D = x.shape\n",
    "        D_in = W3.shape[1]\n",
    "        \n",
    "        # x = x.view(B, D*S)\n",
    "\n",
    "        a = x @ W1.T\n",
    "        b = x @ W2.T\n",
    "        sigma = 1 / (1 + torch.exp(-b))\n",
    "        c = b * sigma\n",
    "        d = a * c\n",
    "\n",
    "        # dW3\n",
    "        dW3 =  grad_output.permute(0, 2, 1) @ d\n",
    "        # dW3 =  grad_output.T @ d\n",
    "\n",
    "        # dW2\n",
    "        act_prime = sigma * (1 + b * (1 - sigma))\n",
    "        dL_db = (((grad_output @ W3) * a) * act_prime)\n",
    "        dW2 = dL_db.permute(0, 2, 1) @ x\n",
    "\n",
    "        # dW1\n",
    "        dL_da = ((grad_output @ W3) * c)\n",
    "        dW1 = dL_da.permute(0, 2, 1) @ x\n",
    "\n",
    "        # dx\n",
    "        dx = dL_da @ W1 + dL_db @ W2\n",
    "\n",
    "        return dx, dW1, dW2, dW3\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d925ee88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class LlamaMLPFunction(Function):\n",
    "#     @staticmethod\n",
    "#     def forward(ctx, x, W1, W2, W3):\n",
    "#         a = x @ W1.T\n",
    "#         b = x @ W2.T\n",
    "#         sigma = 1 / (1 + torch.exp(-b))\n",
    "#         c = b * sigma\n",
    "#         d = a * c\n",
    "#         e = d @ W3.T\n",
    "#         ctx.save_for_backward(x, W1, W2, W3, a, b, c, d, sigma)\n",
    "#         return e\n",
    "    \n",
    "#     @staticmethod\n",
    "#     def backward(ctx, grad_output):\n",
    "#         x, W1, W2, W3, a, b, c, d, sigma = ctx.saved_tensors\n",
    "\n",
    "#         # dW3\n",
    "#         dW3 =  grad_output.T @ d\n",
    "\n",
    "#         # dW2\n",
    "#         act_prime = sigma * (1 + b * (1 - sigma))\n",
    "#         dL_db = (((grad_output @ W3) * a) * act_prime)\n",
    "#         dW2 = dL_db.T @ x\n",
    "        \n",
    "#         # dW1\n",
    "#         dL_da = ((grad_output @ W3) * c)\n",
    "#         dW1 = dL_da.T @ x\n",
    "\n",
    "#         # dx\n",
    "#         dx = dL_da @ W1 + dL_db @ W2\n",
    "\n",
    "#         return dx, dW1, dW2, dW3\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bcc4af8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LlamaMLPFunction_1(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, x, W1, W2, W3):\n",
    "        u = x @ W2.T                      # (B,L,I)\n",
    "        v = x @ W1.T                      # (B,L,I)\n",
    "        h = torch.nn.functional.silu(u)   # (B,L,I)\n",
    "        out = (h * v) @ W3.T              # (B,L,D)\n",
    "        ctx.save_for_backward(x, W1, W2, W3, u, v, h)\n",
    "        return out\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        x, W1, W2, W3, u, v, h= ctx.saved_tensors\n",
    "    \n",
    "\n",
    "        # dW3\n",
    "        # dW3 = torch.einsum(\"bld,bli->di\", grad_output, h * v)\n",
    "        dW3 = (grad_output.permute(0,2,1) @ (h * v)).sum(dim=0)\n",
    "\n",
    "        # dL_dv, dW1\n",
    "        dL_dv = (grad_output @ W3) * h\n",
    "        # dW1 = torch.einsum(\"bli,bld->id\", dL_dv, x)\n",
    "        dW1 = (dL_dv.permute(0,2,1) @ x).sum(dim=0)\n",
    "\n",
    "        # dL_du, dW2\n",
    "        sigma = torch.sigmoid(u)\n",
    "        act_prime = sigma * (1 + u * (1 - sigma))\n",
    "        dL_du = (grad_output @ W3) * v * act_prime\n",
    "        # dW2 = torch.einsum(\"bli,bld->id\", dL_du, x)\n",
    "        dW2 = (dL_du.permute(0,2,1) @ x).sum(dim=0)\n",
    "\n",
    "        # dx\n",
    "        dx = dL_dv @ W1 + dL_du @ W2\n",
    "\n",
    "        return dx, dW1, dW2, dW3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "777cee6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LlamaMLPFunction_2(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, x, W1, W2, W3):\n",
    "        u = x @ W2.T                      # (B,L,I)\n",
    "        v = x @ W1.T                      # (B,L,I)\n",
    "        h = torch.nn.functional.silu(u)   # (B,L,I)\n",
    "        out = (h * v) @ W3.T              # (B,L,D)\n",
    "        ctx.save_for_backward(x, W1, W2, W3)\n",
    "        return out\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        x, W1, W2, W3= ctx.saved_tensors\n",
    "        \n",
    "        # recompute activations (Save memory)\n",
    "        u = x @ W2.T                      # (B,L,I)\n",
    "        v = x @ W1.T                      # (B,L,I)\n",
    "        h = torch.nn.functional.silu(u)   # (B,L,I)\n",
    "\n",
    "        # dW3\n",
    "        dW3 = torch.einsum(\"bld,bli->di\", grad_output, h * v)\n",
    "\n",
    "        # dL_dv, dW1\n",
    "        dL_dv = (grad_output @ W3) * h\n",
    "        dW1 = torch.einsum(\"bli,bld->id\", dL_dv, x)\n",
    "\n",
    "        # dL_du, dW2\n",
    "        sigma = torch.sigmoid(u)\n",
    "        act_prime = sigma * (1 + u * (1 - sigma))\n",
    "        dL_du = (grad_output @ W3) * v * act_prime\n",
    "        dW2 = torch.einsum(\"bli,bld->id\", dL_du, x)\n",
    "\n",
    "        # dx\n",
    "        dx = dL_dv @ W1 + dL_du @ W2\n",
    "\n",
    "        return dx, dW1, dW2, dW3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "e16f3389",
   "metadata": {
    "executionInfo": {
     "elapsed": 63,
     "status": "ok",
     "timestamp": 1757603966211,
     "user": {
      "displayName": "Matteo Bonotto",
      "userId": "01035356913648804886"
     },
     "user_tz": -120
    },
    "id": "e16f3389"
   },
   "outputs": [],
   "source": [
    "B, S, D = 4, 1000, 128\n",
    "x = torch.rand(B, S, D, requires_grad=True)\n",
    "\n",
    "mlp = LlamaMLP(hidden_size=D)\n",
    "\n",
    "assert torch.allclose(\n",
    "    mlp(x),\n",
    "    LlamaMLPFunction.apply(x, mlp.W1.weight, mlp.W2.weight, mlp.W3.weight),\n",
    "    atol=1e-7\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "789e9f9f",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 365
    },
    "executionInfo": {
     "elapsed": 200,
     "status": "error",
     "timestamp": 1757603971204,
     "user": {
      "displayName": "Matteo Bonotto",
      "userId": "01035356913648804886"
     },
     "user_tz": -120
    },
    "id": "789e9f9f",
    "outputId": "20dafc8e-2ac4-4659-87fe-7eb550c104cf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient 0: max diff = 7.45e-08\n",
      "Gradient 1: max diff = 0.0\n",
      "Gradient 2: max diff = 7.63e-06\n",
      "Gradient 3: max diff = 0.0\n"
     ]
    }
   ],
   "source": [
    "### Check backward pass\n",
    "\n",
    "\n",
    "# Forward pass\n",
    "out_a = LlamaMLPFunction.apply(x, mlp.W1.weight, mlp.W2.weight, mlp.W3.weight)\n",
    "out_b = mlp_fwd(x, mlp.W1.weight, mlp.W2.weight, mlp.W3.weight)\n",
    "\n",
    "grad_output = torch.randn_like(out_a)\n",
    "\n",
    "# Backward pass A\n",
    "grads_a = torch.autograd.grad(out_a, (x, mlp.W1.weight, mlp.W2.weight, mlp.W3.weight), grad_outputs=grad_output, retain_graph=True)\n",
    "\n",
    "# Backward pass B\n",
    "grads_b = torch.autograd.grad(out_b, (x, mlp.W1.weight, mlp.W2.weight, mlp.W3.weight), grad_outputs=grad_output, retain_graph=True)\n",
    "\n",
    "for i, (ga, gb) in enumerate(zip(grads_a, grads_b)):\n",
    "    max_diff = (ga - gb).abs().max().item()\n",
    "    print(f\"Gradient {i}: max diff = {max_diff:3.3}\")\n",
    "    # assert torch.allclose(ga, gb, atol=1e-6), f\"Mismatch in grad {i}\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "R6rYQ7swWrow",
   "metadata": {
    "id": "R6rYQ7swWrow"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
