{"cells":[{"cell_type":"code","execution_count":33,"id":"96b50ed1","metadata":{"executionInfo":{"elapsed":17,"status":"ok","timestamp":1757606050033,"user":{"displayName":"Matteo Bonotto","userId":"01035356913648804886"},"user_tz":-120},"id":"96b50ed1","colab":{"base_uri":"https://localhost:8080/"},"outputId":"3b080077-e7af-48db-a14b-dee40d8a1c01"},"outputs":[{"output_type":"stream","name":"stdout","text":["DEVICE=device(type='cuda')\n"]}],"source":["import torch\n","from torch import nn, Tensor\n","from torch.autograd import Function\n","\n","if torch.cuda.is_available():\n","    DEVICE = torch.device('cuda')\n","else:\n","    DEVICE = torch.device('cpu')\n","\n","print(f'{DEVICE=}')"]},{"cell_type":"code","execution_count":15,"id":"0149bd2d","metadata":{"executionInfo":{"elapsed":238,"status":"ok","timestamp":1757605762309,"user":{"displayName":"Matteo Bonotto","userId":"01035356913648804886"},"user_tz":-120},"id":"0149bd2d"},"outputs":[],"source":["def mlp_fwd(x, W1, W2, W3):\n","    return (torch.nn.functional.silu(x @ W2.T) * (x @ W1.T)) @ W3.T\n","\n","class LlamaMLP(nn.Module):\n","    def __init__(self, hidden_size:int = 64, intermediate_size:int = 256):\n","        super().__init__()\n","        self.W1 = nn.Linear(hidden_size, intermediate_size, bias=False)\n","        self.W2 = nn.Linear(hidden_size, intermediate_size, bias=False)\n","        self.W3 = nn.Linear(intermediate_size, hidden_size, bias=False)\n","        self.act = nn.SiLU()\n","\n","    def forward(self, x:Tensor) -> Tensor:\n","        return mlp_fwd(x, self.W1.weight, self.W2.weight, self.W3.weight)"]},{"cell_type":"code","execution_count":16,"id":"aea0bba3","metadata":{"executionInfo":{"elapsed":27,"status":"ok","timestamp":1757605762634,"user":{"displayName":"Matteo Bonotto","userId":"01035356913648804886"},"user_tz":-120},"id":"aea0bba3"},"outputs":[],"source":["import torch\n","import os\n","from typing import Dict, Any\n","import psutil\n","from time import time\n","\n","class CUDAMemTracker:\n","    def __init__(self, device=None):\n","        self.device = torch.device(device or \"cuda\")\n","\n","    def __enter__(self):\n","        torch.cuda.synchronize(self.device)\n","        torch.cuda.reset_peak_memory_stats(self.device)\n","        self.start_alloc = torch.cuda.memory_allocated(self.device)\n","        self.start_reserved = torch.cuda.memory_reserved(self.device)\n","        self.start_rss = psutil.Process(os.getpid()).memory_info().rss if psutil else None\n","        return self\n","\n","    def __exit__(self, exc_type, exc, tb):\n","        torch.cuda.synchronize(self.device)\n","        self.end_alloc = torch.cuda.memory_allocated(self.device)\n","        self.end_reserved = torch.cuda.memory_reserved(self.device)\n","        self.peak_alloc = torch.cuda.max_memory_allocated(self.device)\n","        self.peak_reserved = torch.cuda.max_memory_reserved(self.device)\n","\n","    def report(self, unit=1024**2):\n","        to_mb = lambda b: None if b is None else b / unit\n","        return {\n","            \"alloc_delta_MB\": to_mb(self.end_alloc - self.start_alloc),\n","            \"reserved_delta_MB\": to_mb(self.end_reserved - self.start_reserved),\n","            \"peak_alloc_MB\": to_mb(self.peak_alloc),\n","            \"peak_reserved_MB\": to_mb(self.peak_reserved),\n","            \"cpu_rss_MB\": to_mb(self.start_rss) if self.start_rss is not None else None,\n","        }\n","\n","\n","\n","def profile_torch_module_forward(module: torch.nn.Module, inputs: Dict[str, Any]):\n","    t0 = time()\n","    with CUDAMemTracker() as t:\n","        module(**inputs)\n","    t1 = time() - t0\n","\n","    report = t.report()\n","    print(f'Elapsed time:       {t1:3.3}s')\n","    print(f'Peak allocated mem: {report['peak_alloc_MB']:3.3}MB')\n","    print(f'Peak reserved mem:  {report['peak_alloc_MB']:3.3}MB')\n","\n","\n","def profile_torch_module_backward(module: torch.nn.Module, inputs: Dict[str, Any]):\n","    t0 = time()\n","    with CUDAMemTracker() as t:\n","        out = module(**inputs)\n","        loss = out.sum()\n","        loss.backward()\n","    t1 = time() - t0\n","\n","    report = t.report()\n","    print(f'Elapsed time:       {t1:3.3}s')\n","    print(f'Peak allocated mem: {report['peak_alloc_MB']:3.3}MB')\n","    print(f'Peak reserved mem:  {report['peak_alloc_MB']:3.3}MB')\n"]},{"cell_type":"code","execution_count":17,"id":"304f8d88","metadata":{"executionInfo":{"elapsed":22,"status":"ok","timestamp":1757605765256,"user":{"displayName":"Matteo Bonotto","userId":"01035356913648804886"},"user_tz":-120},"id":"304f8d88"},"outputs":[],"source":["class LlamaMLPFunction(Function):\n","    @staticmethod\n","    def forward(ctx, x, W1, W2, W3):\n","        a = x @ W1.T\n","        b = x @ W2.T\n","        sigma = 1 / (1 + torch.exp(-b))\n","        c = b * sigma\n","        d = a * c\n","        e = d @ W3.T\n","        # ctx.save_for_backward(x, W1, W2, W3, a, b, c, d, sigma)\n","        ctx.save_for_backward(x, W1, W2, W3)\n","        return e\n","\n","    @staticmethod\n","    def backward(ctx, grad_output):\n","        x, W1, W2, W3 = ctx.saved_tensors\n","        B, S, D = x.shape\n","        D_in = W3.shape[1]\n","\n","        # x = x.view(B, D*S)\n","\n","        a = x @ W1.T\n","        b = x @ W2.T\n","        sigma = 1 / (1 + torch.exp(-b))\n","        c = b * sigma\n","        d = a * c\n","\n","        # dW3\n","        dW3 =  grad_output.permute(0, 2, 1) @ d\n","        # dW3 =  grad_output.T @ d\n","\n","        # dW2\n","        act_prime = sigma * (1 + b * (1 - sigma))\n","        dL_db = (((grad_output @ W3) * a) * act_prime)\n","        dW2 = dL_db.permute(0, 2, 1) @ x\n","\n","        # dW1\n","        dL_da = ((grad_output @ W3) * c)\n","        dW1 = dL_da.permute(0, 2, 1) @ x\n","\n","        # dx\n","        dx = dL_da @ W1 + dL_db @ W2\n","\n","        return dx, dW1, dW2, dW3\n","\n","\n"]},{"cell_type":"code","execution_count":18,"id":"d925ee88","metadata":{"id":"d925ee88","executionInfo":{"status":"ok","timestamp":1757605766430,"user_tz":-120,"elapsed":16,"user":{"displayName":"Matteo Bonotto","userId":"01035356913648804886"}}},"outputs":[],"source":["# class LlamaMLPFunction(Function):\n","#     @staticmethod\n","#     def forward(ctx, x, W1, W2, W3):\n","#         a = x @ W1.T\n","#         b = x @ W2.T\n","#         sigma = 1 / (1 + torch.exp(-b))\n","#         c = b * sigma\n","#         d = a * c\n","#         e = d @ W3.T\n","#         ctx.save_for_backward(x, W1, W2, W3, a, b, c, d, sigma)\n","#         return e\n","\n","#     @staticmethod\n","#     def backward(ctx, grad_output):\n","#         x, W1, W2, W3, a, b, c, d, sigma = ctx.saved_tensors\n","\n","#         # dW3\n","#         dW3 =  grad_output.T @ d\n","\n","#         # dW2\n","#         act_prime = sigma * (1 + b * (1 - sigma))\n","#         dL_db = (((grad_output @ W3) * a) * act_prime)\n","#         dW2 = dL_db.T @ x\n","\n","#         # dW1\n","#         dL_da = ((grad_output @ W3) * c)\n","#         dW1 = dL_da.T @ x\n","\n","#         # dx\n","#         dx = dL_da @ W1 + dL_db @ W2\n","\n","#         return dx, dW1, dW2, dW3\n","\n"]},{"cell_type":"code","execution_count":19,"id":"1bcc4af8","metadata":{"id":"1bcc4af8","executionInfo":{"status":"ok","timestamp":1757605767970,"user_tz":-120,"elapsed":22,"user":{"displayName":"Matteo Bonotto","userId":"01035356913648804886"}}},"outputs":[],"source":["class LlamaMLPFunction_1(torch.autograd.Function):\n","    @staticmethod\n","    def forward(ctx, x, W1, W2, W3):\n","        u = x @ W2.T                      # (B,L,I)\n","        v = x @ W1.T                      # (B,L,I)\n","        h = torch.nn.functional.silu(u)   # (B,L,I)\n","        out = (h * v) @ W3.T              # (B,L,D)\n","        ctx.save_for_backward(x, W1, W2, W3, u, v, h)\n","        return out\n","\n","    @staticmethod\n","    def backward(ctx, grad_output):\n","        x, W1, W2, W3, u, v, h= ctx.saved_tensors\n","\n","\n","        # dW3\n","        # dW3 = torch.einsum(\"bld,bli->di\", grad_output, h * v)\n","        dW3 = (grad_output.permute(0,2,1) @ (h * v)).sum(dim=0)\n","\n","        # dL_dv, dW1\n","        dL_dv = (grad_output @ W3) * h\n","        # dW1 = torch.einsum(\"bli,bld->id\", dL_dv, x)\n","        dW1 = (dL_dv.permute(0,2,1) @ x).sum(dim=0)\n","\n","        # dL_du, dW2\n","        sigma = torch.sigmoid(u)\n","        act_prime = sigma * (1 + u * (1 - sigma))\n","        dL_du = (grad_output @ W3) * v * act_prime\n","        # dW2 = torch.einsum(\"bli,bld->id\", dL_du, x)\n","        dW2 = (dL_du.permute(0,2,1) @ x).sum(dim=0)\n","\n","        # dx\n","        dx = dL_dv @ W1 + dL_du @ W2\n","\n","        return dx, dW1, dW2, dW3\n"]},{"cell_type":"code","execution_count":66,"id":"777cee6b","metadata":{"id":"777cee6b","executionInfo":{"status":"ok","timestamp":1757606658703,"user_tz":-120,"elapsed":8,"user":{"displayName":"Matteo Bonotto","userId":"01035356913648804886"}}},"outputs":[],"source":["class LlamaMLPFunction_2(torch.autograd.Function):\n","    @staticmethod\n","    def forward(ctx, x, W1, W2, W3):\n","        with torch.no_grad():\n","            u = x @ W2.T                      # (B,L,I)\n","            v = x @ W1.T                      # (B,L,I)\n","            h = torch.nn.functional.silu(u)   # (B,L,I)\n","            out = (h * v) @ W3.T              # (B,L,D)\n","        ctx.save_for_backward(x, W1, W2, W3)\n","        return out\n","\n","    @staticmethod\n","    def backward(ctx, grad_output):\n","        x, W1, W2, W3= ctx.saved_tensors\n","\n","        with torch.no_grad():\n","\n","            # recompute activations (Save memory)\n","            u = x @ W2.T                      # (B,L,I)\n","            v = x @ W1.T                      # (B,L,I)\n","            h = torch.nn.functional.silu(u)   # (B,L,I)\n","\n","            # dW3\n","            dW3 = torch.einsum(\"bld,bli->di\", grad_output, h * v)\n","\n","            # dL_dv, dW1\n","            dL_dv = (grad_output @ W3) * h\n","            dW1 = torch.einsum(\"bli,bld->id\", dL_dv, x)\n","\n","            # dL_du, dW2\n","            sigma = torch.sigmoid(u)\n","            act_prime = sigma * (1 + u * (1 - sigma))\n","            dL_du = (grad_output @ W3) * v * act_prime\n","            dW2 = torch.einsum(\"bli,bld->id\", dL_du, x)\n","\n","            # dx\n","            dx = dL_dv @ W1 + dL_du @ W2\n","\n","        return dx, dW1, dW2, dW3\n"]},{"cell_type":"code","execution_count":67,"id":"e16f3389","metadata":{"executionInfo":{"elapsed":10,"status":"ok","timestamp":1757606658812,"user":{"displayName":"Matteo Bonotto","userId":"01035356913648804886"},"user_tz":-120},"id":"e16f3389"},"outputs":[],"source":["B, S, D = 4, 1000, 128\n","x = torch.rand(B, S, D, requires_grad=True).to(DEVICE)\n","\n","mlp = LlamaMLP(hidden_size=D).to(DEVICE)\n","\n","assert torch.allclose(\n","    mlp(x),\n","    LlamaMLPFunction_2.apply(x, mlp.W1.weight, mlp.W2.weight, mlp.W3.weight),\n","    atol=1e-7\n",")"]},{"cell_type":"code","execution_count":68,"id":"789e9f9f","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":95,"status":"ok","timestamp":1757606659112,"user":{"displayName":"Matteo Bonotto","userId":"01035356913648804886"},"user_tz":-120},"id":"789e9f9f","outputId":"651f0b07-1a32-46e7-81c7-c10b00d35af8"},"outputs":[{"output_type":"stream","name":"stdout","text":["Gradient 0: max diff = 5.96e-08\n","Gradient 1: max diff = 0.0\n","Gradient 2: max diff = 4.77e-06\n","Gradient 3: max diff = 0.0\n"]}],"source":["### Check backward pass\n","\n","\n","# Forward pass\n","out_a = LlamaMLPFunction_2.apply(x, mlp.W1.weight, mlp.W2.weight, mlp.W3.weight)\n","out_b = mlp_fwd(x, mlp.W1.weight, mlp.W2.weight, mlp.W3.weight)\n","\n","grad_output = torch.randn_like(out_a)\n","\n","# Backward pass A\n","grads_a = torch.autograd.grad(out_a, (x, mlp.W1.weight, mlp.W2.weight, mlp.W3.weight), grad_outputs=grad_output, retain_graph=True)\n","\n","# Backward pass B\n","grads_b = torch.autograd.grad(out_b, (x, mlp.W1.weight, mlp.W2.weight, mlp.W3.weight), grad_outputs=grad_output, retain_graph=True)\n","\n","for i, (ga, gb) in enumerate(zip(grads_a, grads_b)):\n","    max_diff = (ga - gb).abs().max().item()\n","    print(f\"Gradient {i}: max diff = {max_diff:3.3}\")\n","    # assert torch.allclose(ga, gb, atol=1e-6), f\"Mismatch in grad {i}\"\n","\n"]},{"cell_type":"code","execution_count":69,"id":"R6rYQ7swWrow","metadata":{"id":"R6rYQ7swWrow","executionInfo":{"status":"ok","timestamp":1757606659261,"user_tz":-120,"elapsed":41,"user":{"displayName":"Matteo Bonotto","userId":"01035356913648804886"}}},"outputs":[],"source":["class LlamaMLPmanual(nn.Module):\n","    def __init__(self, hidden_size:int = 64, intermediate_size:int = 256):\n","        super().__init__()\n","        self.W1 = nn.Linear(hidden_size, intermediate_size, bias=False)\n","        self.W2 = nn.Linear(hidden_size, intermediate_size, bias=False)\n","        self.W3 = nn.Linear(intermediate_size, hidden_size, bias=False)\n","        self.act = nn.SiLU()\n","\n","    def forward(self, x:Tensor) -> Tensor:\n","        return LlamaMLPFunction_2.apply(x, self.W1.weight, self.W2.weight, self.W3.weight)\n","\n","mlp_manual = LlamaMLPmanual(hidden_size=D).to(DEVICE)\n"]},{"cell_type":"code","source":["inputs = {'x' : x}\n","profile_torch_module_backward(mlp, inputs)\n","\n","print(' ')\n","profile_torch_module_backward(mlp_manual, inputs)\n","\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Tl3aKFuneM0j","executionInfo":{"status":"ok","timestamp":1757606659800,"user_tz":-120,"elapsed":13,"user":{"displayName":"Matteo Bonotto","userId":"01035356913648804886"}},"outputId":"3346c854-41c7-4b72-b6a4-52a1170d3d77"},"id":"Tl3aKFuneM0j","execution_count":70,"outputs":[{"output_type":"stream","name":"stdout","text":["Elapsed time:       0.00483s\n","Peak allocated mem: 75.8MB\n","Peak reserved mem:  75.8MB\n"," \n","Elapsed time:       0.00484s\n","Peak allocated mem: 87.3MB\n","Peak reserved mem:  87.3MB\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"g9qxwvoZfJVX"},"id":"g9qxwvoZfJVX","execution_count":null,"outputs":[]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.17"}},"nbformat":4,"nbformat_minor":5}