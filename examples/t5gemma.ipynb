{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b5daacf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers.models.t5gemma.configuration_t5gemma import T5GemmaConfig\n",
    "from transformers.models.t5gemma.modeling_t5gemma import T5GemmaForConditionalGeneration\n",
    "from transformers import AutoModelForSeq2SeqLM\n",
    "from torchinfo import summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d91a60f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file config.json from cache at /Users/matte/.cache/huggingface/hub/models--google--t5gemma-b-b-prefixlm/snapshots/df5e1422c2a53948a57901c54bfcf397c92bfa1d/config.json\n",
      "Model config T5GemmaConfig {\n",
      "  \"architectures\": [\n",
      "    \"T5GemmaForConditionalGeneration\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 2,\n",
      "  \"classifier_dropout_rate\": 0.0,\n",
      "  \"decoder\": {\n",
      "    \"attention_bias\": false,\n",
      "    \"attention_dropout\": 0.0,\n",
      "    \"attn_logit_softcapping\": 50.0,\n",
      "    \"classifier_dropout_rate\": 0.0,\n",
      "    \"cross_attention_hidden_size\": 768,\n",
      "    \"dropout_rate\": 0.0,\n",
      "    \"dtype\": \"bfloat16\",\n",
      "    \"final_logit_softcapping\": 30.0,\n",
      "    \"head_dim\": 64,\n",
      "    \"hidden_activation\": \"gelu_pytorch_tanh\",\n",
      "    \"hidden_size\": 768,\n",
      "    \"initializer_range\": 0.02,\n",
      "    \"intermediate_size\": 2048,\n",
      "    \"is_decoder\": true,\n",
      "    \"layer_types\": [\n",
      "      \"sliding_attention\",\n",
      "      \"full_attention\",\n",
      "      \"sliding_attention\",\n",
      "      \"full_attention\",\n",
      "      \"sliding_attention\",\n",
      "      \"full_attention\",\n",
      "      \"sliding_attention\",\n",
      "      \"full_attention\",\n",
      "      \"sliding_attention\",\n",
      "      \"full_attention\",\n",
      "      \"sliding_attention\",\n",
      "      \"full_attention\"\n",
      "    ],\n",
      "    \"max_position_embeddings\": 8192,\n",
      "    \"model_type\": \"t5_gemma_module\",\n",
      "    \"num_attention_heads\": 12,\n",
      "    \"num_hidden_layers\": 12,\n",
      "    \"num_key_value_heads\": 12,\n",
      "    \"query_pre_attn_scalar\": 64,\n",
      "    \"rms_norm_eps\": 1e-06,\n",
      "    \"rope_theta\": 10000.0,\n",
      "    \"sliding_window\": 4096,\n",
      "    \"use_cache\": true,\n",
      "    \"vocab_size\": 256000\n",
      "  },\n",
      "  \"dropout_rate\": 0.0,\n",
      "  \"dtype\": \"bfloat16\",\n",
      "  \"encoder\": {\n",
      "    \"attention_bias\": false,\n",
      "    \"attention_dropout\": 0.0,\n",
      "    \"attn_logit_softcapping\": 50.0,\n",
      "    \"classifier_dropout_rate\": 0.0,\n",
      "    \"dropout_rate\": 0.0,\n",
      "    \"dtype\": \"bfloat16\",\n",
      "    \"final_logit_softcapping\": 30.0,\n",
      "    \"head_dim\": 64,\n",
      "    \"hidden_activation\": \"gelu_pytorch_tanh\",\n",
      "    \"hidden_size\": 768,\n",
      "    \"initializer_range\": 0.02,\n",
      "    \"intermediate_size\": 2048,\n",
      "    \"layer_types\": [\n",
      "      \"sliding_attention\",\n",
      "      \"full_attention\",\n",
      "      \"sliding_attention\",\n",
      "      \"full_attention\",\n",
      "      \"sliding_attention\",\n",
      "      \"full_attention\",\n",
      "      \"sliding_attention\",\n",
      "      \"full_attention\",\n",
      "      \"sliding_attention\",\n",
      "      \"full_attention\",\n",
      "      \"sliding_attention\",\n",
      "      \"full_attention\"\n",
      "    ],\n",
      "    \"max_position_embeddings\": 8192,\n",
      "    \"model_type\": \"t5_gemma_module\",\n",
      "    \"num_attention_heads\": 12,\n",
      "    \"num_hidden_layers\": 12,\n",
      "    \"num_key_value_heads\": 12,\n",
      "    \"query_pre_attn_scalar\": 64,\n",
      "    \"rms_norm_eps\": 1e-06,\n",
      "    \"rope_theta\": 10000.0,\n",
      "    \"sliding_window\": 4096,\n",
      "    \"use_cache\": true,\n",
      "    \"vocab_size\": 256000\n",
      "  },\n",
      "  \"eos_token_id\": [\n",
      "    1,\n",
      "    107\n",
      "  ],\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"model_type\": \"t5gemma\",\n",
      "  \"pad_token_id\": 0,\n",
      "  \"transformers_version\": \"4.56.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 256000\n",
      "}\n",
      "\n",
      "loading weights file model.safetensors from cache at /Users/matte/.cache/huggingface/hub/models--google--t5gemma-b-b-prefixlm/snapshots/df5e1422c2a53948a57901c54bfcf397c92bfa1d/model.safetensors\n",
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 2,\n",
      "  \"eos_token_id\": [\n",
      "    1,\n",
      "    107\n",
      "  ],\n",
      "  \"pad_token_id\": 0\n",
      "}\n",
      "\n",
      "All model checkpoint weights were used when initializing T5GemmaForConditionalGeneration.\n",
      "\n",
      "All the weights of T5GemmaForConditionalGeneration were initialized from the model checkpoint at google/t5gemma-b-b-prefixlm.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use T5GemmaForConditionalGeneration for predictions without further training.\n",
      "loading configuration file generation_config.json from cache at /Users/matte/.cache/huggingface/hub/models--google--t5gemma-b-b-prefixlm/snapshots/df5e1422c2a53948a57901c54bfcf397c92bfa1d/generation_config.json\n",
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 2,\n",
      "  \"eos_token_id\": [\n",
      "    1,\n",
      "    107\n",
      "  ],\n",
      "  \"pad_token_id\": 0\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"google/t5gemma-b-b-prefixlm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e38b9cc3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "T5GemmaForConditionalGeneration(\n",
       "  (model): T5GemmaModel(\n",
       "    (encoder): T5GemmaEncoder(\n",
       "      (embed_tokens): Embedding(256000, 768, padding_idx=0)\n",
       "      (norm): T5GemmaRMSNorm((768,), eps=1e-06)\n",
       "      (rotary_emb): T5GemmaRotaryEmbedding()\n",
       "      (layers): ModuleList(\n",
       "        (0-11): 12 x T5GemmaEncoderLayer(\n",
       "          (self_attn): T5GemmaSelfAttention(\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (o_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "          )\n",
       "          (pre_self_attn_layernorm): T5GemmaRMSNorm((768,), eps=1e-06)\n",
       "          (post_self_attn_layernorm): T5GemmaRMSNorm((768,), eps=1e-06)\n",
       "          (mlp): T5GemmaMLP(\n",
       "            (gate_proj): Linear(in_features=768, out_features=2048, bias=False)\n",
       "            (up_proj): Linear(in_features=768, out_features=2048, bias=False)\n",
       "            (down_proj): Linear(in_features=2048, out_features=768, bias=False)\n",
       "            (act_fn): PytorchGELUTanh()\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (pre_feedforward_layernorm): T5GemmaRMSNorm((768,), eps=1e-06)\n",
       "          (post_feedforward_layernorm): T5GemmaRMSNorm((768,), eps=1e-06)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (dropout): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (decoder): T5GemmaDecoder(\n",
       "      (embed_tokens): Embedding(256000, 768, padding_idx=0)\n",
       "      (norm): T5GemmaRMSNorm((768,), eps=1e-06)\n",
       "      (rotary_emb): T5GemmaRotaryEmbedding()\n",
       "      (layers): ModuleList(\n",
       "        (0-11): 12 x T5GemmaDecoderLayer(\n",
       "          (self_attn): T5GemmaSelfAttention(\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (o_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "          )\n",
       "          (pre_self_attn_layernorm): T5GemmaRMSNorm((768,), eps=1e-06)\n",
       "          (post_self_attn_layernorm): T5GemmaRMSNorm((768,), eps=1e-06)\n",
       "          (mlp): T5GemmaMLP(\n",
       "            (gate_proj): Linear(in_features=768, out_features=2048, bias=False)\n",
       "            (up_proj): Linear(in_features=768, out_features=2048, bias=False)\n",
       "            (down_proj): Linear(in_features=2048, out_features=768, bias=False)\n",
       "            (act_fn): PytorchGELUTanh()\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (pre_feedforward_layernorm): T5GemmaRMSNorm((768,), eps=1e-06)\n",
       "          (post_feedforward_layernorm): T5GemmaRMSNorm((768,), eps=1e-06)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "          (cross_attn): T5GemmaCrossAttention(\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (o_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "          )\n",
       "          (pre_cross_attn_layernorm): T5GemmaRMSNorm((768,), eps=1e-06)\n",
       "          (post_cross_attn_layernorm): T5GemmaRMSNorm((768,), eps=1e-06)\n",
       "        )\n",
       "      )\n",
       "      (dropout): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "  )\n",
       "  (lm_head): T5GemmaLMHead(\n",
       "    (out_proj): Linear(in_features=768, out_features=256000, bias=False)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "57b6e5f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "================================================================================\n",
       "Layer (type:depth-idx)                                  Param #\n",
       "================================================================================\n",
       "T5GemmaForConditionalGeneration                         --\n",
       "├─T5GemmaModel: 1-1                                     --\n",
       "│    └─T5GemmaEncoder: 2-1                              --\n",
       "│    │    └─Embedding: 3-1                              196,608,000\n",
       "│    │    └─T5GemmaRMSNorm: 3-2                         768\n",
       "│    │    └─T5GemmaRotaryEmbedding: 3-3                 --\n",
       "│    │    └─ModuleList: 3-4                             --\n",
       "│    │    │    └─T5GemmaEncoderLayer: 4-1               --\n",
       "│    │    │    │    └─T5GemmaSelfAttention: 5-1         --\n",
       "│    │    │    │    │    └─Linear: 6-1                  589,824\n",
       "│    │    │    │    │    └─Linear: 6-2                  589,824\n",
       "│    │    │    │    │    └─Linear: 6-3                  589,824\n",
       "│    │    │    │    │    └─Linear: 6-4                  589,824\n",
       "│    │    │    │    └─T5GemmaRMSNorm: 5-2               768\n",
       "│    │    │    │    └─T5GemmaRMSNorm: 5-3               768\n",
       "│    │    │    │    └─T5GemmaMLP: 5-4                   --\n",
       "│    │    │    │    │    └─Linear: 6-5                  1,572,864\n",
       "│    │    │    │    │    └─Linear: 6-6                  1,572,864\n",
       "│    │    │    │    │    └─Linear: 6-7                  1,572,864\n",
       "│    │    │    │    │    └─PytorchGELUTanh: 6-8         --\n",
       "│    │    │    │    │    └─Dropout: 6-9                 --\n",
       "│    │    │    │    └─T5GemmaRMSNorm: 5-5               768\n",
       "│    │    │    │    └─T5GemmaRMSNorm: 5-6               768\n",
       "│    │    │    │    └─Dropout: 5-7                      --\n",
       "│    │    │    └─T5GemmaEncoderLayer: 4-2               --\n",
       "│    │    │    │    └─T5GemmaSelfAttention: 5-8         --\n",
       "│    │    │    │    │    └─Linear: 6-10                 589,824\n",
       "│    │    │    │    │    └─Linear: 6-11                 589,824\n",
       "│    │    │    │    │    └─Linear: 6-12                 589,824\n",
       "│    │    │    │    │    └─Linear: 6-13                 589,824\n",
       "│    │    │    │    └─T5GemmaRMSNorm: 5-9               768\n",
       "│    │    │    │    └─T5GemmaRMSNorm: 5-10              768\n",
       "│    │    │    │    └─T5GemmaMLP: 5-11                  --\n",
       "│    │    │    │    │    └─Linear: 6-14                 1,572,864\n",
       "│    │    │    │    │    └─Linear: 6-15                 1,572,864\n",
       "│    │    │    │    │    └─Linear: 6-16                 1,572,864\n",
       "│    │    │    │    │    └─PytorchGELUTanh: 6-17        --\n",
       "│    │    │    │    │    └─Dropout: 6-18                --\n",
       "│    │    │    │    └─T5GemmaRMSNorm: 5-12              768\n",
       "│    │    │    │    └─T5GemmaRMSNorm: 5-13              768\n",
       "│    │    │    │    └─Dropout: 5-14                     --\n",
       "│    │    │    └─T5GemmaEncoderLayer: 4-3               --\n",
       "│    │    │    │    └─T5GemmaSelfAttention: 5-15        --\n",
       "│    │    │    │    │    └─Linear: 6-19                 589,824\n",
       "│    │    │    │    │    └─Linear: 6-20                 589,824\n",
       "│    │    │    │    │    └─Linear: 6-21                 589,824\n",
       "│    │    │    │    │    └─Linear: 6-22                 589,824\n",
       "│    │    │    │    └─T5GemmaRMSNorm: 5-16              768\n",
       "│    │    │    │    └─T5GemmaRMSNorm: 5-17              768\n",
       "│    │    │    │    └─T5GemmaMLP: 5-18                  --\n",
       "│    │    │    │    │    └─Linear: 6-23                 1,572,864\n",
       "│    │    │    │    │    └─Linear: 6-24                 1,572,864\n",
       "│    │    │    │    │    └─Linear: 6-25                 1,572,864\n",
       "│    │    │    │    │    └─PytorchGELUTanh: 6-26        --\n",
       "│    │    │    │    │    └─Dropout: 6-27                --\n",
       "│    │    │    │    └─T5GemmaRMSNorm: 5-19              768\n",
       "│    │    │    │    └─T5GemmaRMSNorm: 5-20              768\n",
       "│    │    │    │    └─Dropout: 5-21                     --\n",
       "│    │    │    └─T5GemmaEncoderLayer: 4-4               --\n",
       "│    │    │    │    └─T5GemmaSelfAttention: 5-22        --\n",
       "│    │    │    │    │    └─Linear: 6-28                 589,824\n",
       "│    │    │    │    │    └─Linear: 6-29                 589,824\n",
       "│    │    │    │    │    └─Linear: 6-30                 589,824\n",
       "│    │    │    │    │    └─Linear: 6-31                 589,824\n",
       "│    │    │    │    └─T5GemmaRMSNorm: 5-23              768\n",
       "│    │    │    │    └─T5GemmaRMSNorm: 5-24              768\n",
       "│    │    │    │    └─T5GemmaMLP: 5-25                  --\n",
       "│    │    │    │    │    └─Linear: 6-32                 1,572,864\n",
       "│    │    │    │    │    └─Linear: 6-33                 1,572,864\n",
       "│    │    │    │    │    └─Linear: 6-34                 1,572,864\n",
       "│    │    │    │    │    └─PytorchGELUTanh: 6-35        --\n",
       "│    │    │    │    │    └─Dropout: 6-36                --\n",
       "│    │    │    │    └─T5GemmaRMSNorm: 5-26              768\n",
       "│    │    │    │    └─T5GemmaRMSNorm: 5-27              768\n",
       "│    │    │    │    └─Dropout: 5-28                     --\n",
       "│    │    │    └─T5GemmaEncoderLayer: 4-5               --\n",
       "│    │    │    │    └─T5GemmaSelfAttention: 5-29        --\n",
       "│    │    │    │    │    └─Linear: 6-37                 589,824\n",
       "│    │    │    │    │    └─Linear: 6-38                 589,824\n",
       "│    │    │    │    │    └─Linear: 6-39                 589,824\n",
       "│    │    │    │    │    └─Linear: 6-40                 589,824\n",
       "│    │    │    │    └─T5GemmaRMSNorm: 5-30              768\n",
       "│    │    │    │    └─T5GemmaRMSNorm: 5-31              768\n",
       "│    │    │    │    └─T5GemmaMLP: 5-32                  --\n",
       "│    │    │    │    │    └─Linear: 6-41                 1,572,864\n",
       "│    │    │    │    │    └─Linear: 6-42                 1,572,864\n",
       "│    │    │    │    │    └─Linear: 6-43                 1,572,864\n",
       "│    │    │    │    │    └─PytorchGELUTanh: 6-44        --\n",
       "│    │    │    │    │    └─Dropout: 6-45                --\n",
       "│    │    │    │    └─T5GemmaRMSNorm: 5-33              768\n",
       "│    │    │    │    └─T5GemmaRMSNorm: 5-34              768\n",
       "│    │    │    │    └─Dropout: 5-35                     --\n",
       "│    │    │    └─T5GemmaEncoderLayer: 4-6               --\n",
       "│    │    │    │    └─T5GemmaSelfAttention: 5-36        --\n",
       "│    │    │    │    │    └─Linear: 6-46                 589,824\n",
       "│    │    │    │    │    └─Linear: 6-47                 589,824\n",
       "│    │    │    │    │    └─Linear: 6-48                 589,824\n",
       "│    │    │    │    │    └─Linear: 6-49                 589,824\n",
       "│    │    │    │    └─T5GemmaRMSNorm: 5-37              768\n",
       "│    │    │    │    └─T5GemmaRMSNorm: 5-38              768\n",
       "│    │    │    │    └─T5GemmaMLP: 5-39                  --\n",
       "│    │    │    │    │    └─Linear: 6-50                 1,572,864\n",
       "│    │    │    │    │    └─Linear: 6-51                 1,572,864\n",
       "│    │    │    │    │    └─Linear: 6-52                 1,572,864\n",
       "│    │    │    │    │    └─PytorchGELUTanh: 6-53        --\n",
       "│    │    │    │    │    └─Dropout: 6-54                --\n",
       "│    │    │    │    └─T5GemmaRMSNorm: 5-40              768\n",
       "│    │    │    │    └─T5GemmaRMSNorm: 5-41              768\n",
       "│    │    │    │    └─Dropout: 5-42                     --\n",
       "│    │    │    └─T5GemmaEncoderLayer: 4-7               --\n",
       "│    │    │    │    └─T5GemmaSelfAttention: 5-43        --\n",
       "│    │    │    │    │    └─Linear: 6-55                 589,824\n",
       "│    │    │    │    │    └─Linear: 6-56                 589,824\n",
       "│    │    │    │    │    └─Linear: 6-57                 589,824\n",
       "│    │    │    │    │    └─Linear: 6-58                 589,824\n",
       "│    │    │    │    └─T5GemmaRMSNorm: 5-44              768\n",
       "│    │    │    │    └─T5GemmaRMSNorm: 5-45              768\n",
       "│    │    │    │    └─T5GemmaMLP: 5-46                  --\n",
       "│    │    │    │    │    └─Linear: 6-59                 1,572,864\n",
       "│    │    │    │    │    └─Linear: 6-60                 1,572,864\n",
       "│    │    │    │    │    └─Linear: 6-61                 1,572,864\n",
       "│    │    │    │    │    └─PytorchGELUTanh: 6-62        --\n",
       "│    │    │    │    │    └─Dropout: 6-63                --\n",
       "│    │    │    │    └─T5GemmaRMSNorm: 5-47              768\n",
       "│    │    │    │    └─T5GemmaRMSNorm: 5-48              768\n",
       "│    │    │    │    └─Dropout: 5-49                     --\n",
       "│    │    │    └─T5GemmaEncoderLayer: 4-8               --\n",
       "│    │    │    │    └─T5GemmaSelfAttention: 5-50        --\n",
       "│    │    │    │    │    └─Linear: 6-64                 589,824\n",
       "│    │    │    │    │    └─Linear: 6-65                 589,824\n",
       "│    │    │    │    │    └─Linear: 6-66                 589,824\n",
       "│    │    │    │    │    └─Linear: 6-67                 589,824\n",
       "│    │    │    │    └─T5GemmaRMSNorm: 5-51              768\n",
       "│    │    │    │    └─T5GemmaRMSNorm: 5-52              768\n",
       "│    │    │    │    └─T5GemmaMLP: 5-53                  --\n",
       "│    │    │    │    │    └─Linear: 6-68                 1,572,864\n",
       "│    │    │    │    │    └─Linear: 6-69                 1,572,864\n",
       "│    │    │    │    │    └─Linear: 6-70                 1,572,864\n",
       "│    │    │    │    │    └─PytorchGELUTanh: 6-71        --\n",
       "│    │    │    │    │    └─Dropout: 6-72                --\n",
       "│    │    │    │    └─T5GemmaRMSNorm: 5-54              768\n",
       "│    │    │    │    └─T5GemmaRMSNorm: 5-55              768\n",
       "│    │    │    │    └─Dropout: 5-56                     --\n",
       "│    │    │    └─T5GemmaEncoderLayer: 4-9               --\n",
       "│    │    │    │    └─T5GemmaSelfAttention: 5-57        --\n",
       "│    │    │    │    │    └─Linear: 6-73                 589,824\n",
       "│    │    │    │    │    └─Linear: 6-74                 589,824\n",
       "│    │    │    │    │    └─Linear: 6-75                 589,824\n",
       "│    │    │    │    │    └─Linear: 6-76                 589,824\n",
       "│    │    │    │    └─T5GemmaRMSNorm: 5-58              768\n",
       "│    │    │    │    └─T5GemmaRMSNorm: 5-59              768\n",
       "│    │    │    │    └─T5GemmaMLP: 5-60                  --\n",
       "│    │    │    │    │    └─Linear: 6-77                 1,572,864\n",
       "│    │    │    │    │    └─Linear: 6-78                 1,572,864\n",
       "│    │    │    │    │    └─Linear: 6-79                 1,572,864\n",
       "│    │    │    │    │    └─PytorchGELUTanh: 6-80        --\n",
       "│    │    │    │    │    └─Dropout: 6-81                --\n",
       "│    │    │    │    └─T5GemmaRMSNorm: 5-61              768\n",
       "│    │    │    │    └─T5GemmaRMSNorm: 5-62              768\n",
       "│    │    │    │    └─Dropout: 5-63                     --\n",
       "│    │    │    └─T5GemmaEncoderLayer: 4-10              --\n",
       "│    │    │    │    └─T5GemmaSelfAttention: 5-64        --\n",
       "│    │    │    │    │    └─Linear: 6-82                 589,824\n",
       "│    │    │    │    │    └─Linear: 6-83                 589,824\n",
       "│    │    │    │    │    └─Linear: 6-84                 589,824\n",
       "│    │    │    │    │    └─Linear: 6-85                 589,824\n",
       "│    │    │    │    └─T5GemmaRMSNorm: 5-65              768\n",
       "│    │    │    │    └─T5GemmaRMSNorm: 5-66              768\n",
       "│    │    │    │    └─T5GemmaMLP: 5-67                  --\n",
       "│    │    │    │    │    └─Linear: 6-86                 1,572,864\n",
       "│    │    │    │    │    └─Linear: 6-87                 1,572,864\n",
       "│    │    │    │    │    └─Linear: 6-88                 1,572,864\n",
       "│    │    │    │    │    └─PytorchGELUTanh: 6-89        --\n",
       "│    │    │    │    │    └─Dropout: 6-90                --\n",
       "│    │    │    │    └─T5GemmaRMSNorm: 5-68              768\n",
       "│    │    │    │    └─T5GemmaRMSNorm: 5-69              768\n",
       "│    │    │    │    └─Dropout: 5-70                     --\n",
       "│    │    │    └─T5GemmaEncoderLayer: 4-11              --\n",
       "│    │    │    │    └─T5GemmaSelfAttention: 5-71        --\n",
       "│    │    │    │    │    └─Linear: 6-91                 589,824\n",
       "│    │    │    │    │    └─Linear: 6-92                 589,824\n",
       "│    │    │    │    │    └─Linear: 6-93                 589,824\n",
       "│    │    │    │    │    └─Linear: 6-94                 589,824\n",
       "│    │    │    │    └─T5GemmaRMSNorm: 5-72              768\n",
       "│    │    │    │    └─T5GemmaRMSNorm: 5-73              768\n",
       "│    │    │    │    └─T5GemmaMLP: 5-74                  --\n",
       "│    │    │    │    │    └─Linear: 6-95                 1,572,864\n",
       "│    │    │    │    │    └─Linear: 6-96                 1,572,864\n",
       "│    │    │    │    │    └─Linear: 6-97                 1,572,864\n",
       "│    │    │    │    │    └─PytorchGELUTanh: 6-98        --\n",
       "│    │    │    │    │    └─Dropout: 6-99                --\n",
       "│    │    │    │    └─T5GemmaRMSNorm: 5-75              768\n",
       "│    │    │    │    └─T5GemmaRMSNorm: 5-76              768\n",
       "│    │    │    │    └─Dropout: 5-77                     --\n",
       "│    │    │    └─T5GemmaEncoderLayer: 4-12              --\n",
       "│    │    │    │    └─T5GemmaSelfAttention: 5-78        --\n",
       "│    │    │    │    │    └─Linear: 6-100                589,824\n",
       "│    │    │    │    │    └─Linear: 6-101                589,824\n",
       "│    │    │    │    │    └─Linear: 6-102                589,824\n",
       "│    │    │    │    │    └─Linear: 6-103                589,824\n",
       "│    │    │    │    └─T5GemmaRMSNorm: 5-79              768\n",
       "│    │    │    │    └─T5GemmaRMSNorm: 5-80              768\n",
       "│    │    │    │    └─T5GemmaMLP: 5-81                  --\n",
       "│    │    │    │    │    └─Linear: 6-104                1,572,864\n",
       "│    │    │    │    │    └─Linear: 6-105                1,572,864\n",
       "│    │    │    │    │    └─Linear: 6-106                1,572,864\n",
       "│    │    │    │    │    └─PytorchGELUTanh: 6-107       --\n",
       "│    │    │    │    │    └─Dropout: 6-108               --\n",
       "│    │    │    │    └─T5GemmaRMSNorm: 5-82              768\n",
       "│    │    │    │    └─T5GemmaRMSNorm: 5-83              768\n",
       "│    │    │    │    └─Dropout: 5-84                     --\n",
       "│    │    └─Dropout: 3-5                                --\n",
       "│    └─T5GemmaDecoder: 2-2                              --\n",
       "│    │    └─Embedding: 3-6                              196,608,000\n",
       "│    │    └─T5GemmaRMSNorm: 3-7                         768\n",
       "│    │    └─T5GemmaRotaryEmbedding: 3-8                 --\n",
       "│    │    └─ModuleList: 3-9                             --\n",
       "│    │    │    └─T5GemmaDecoderLayer: 4-13              --\n",
       "│    │    │    │    └─T5GemmaSelfAttention: 5-85        --\n",
       "│    │    │    │    │    └─Linear: 6-109                589,824\n",
       "│    │    │    │    │    └─Linear: 6-110                589,824\n",
       "│    │    │    │    │    └─Linear: 6-111                589,824\n",
       "│    │    │    │    │    └─Linear: 6-112                589,824\n",
       "│    │    │    │    └─T5GemmaRMSNorm: 5-86              768\n",
       "│    │    │    │    └─T5GemmaRMSNorm: 5-87              768\n",
       "│    │    │    │    └─T5GemmaMLP: 5-88                  --\n",
       "│    │    │    │    │    └─Linear: 6-113                1,572,864\n",
       "│    │    │    │    │    └─Linear: 6-114                1,572,864\n",
       "│    │    │    │    │    └─Linear: 6-115                1,572,864\n",
       "│    │    │    │    │    └─PytorchGELUTanh: 6-116       --\n",
       "│    │    │    │    │    └─Dropout: 6-117               --\n",
       "│    │    │    │    └─T5GemmaRMSNorm: 5-89              768\n",
       "│    │    │    │    └─T5GemmaRMSNorm: 5-90              768\n",
       "│    │    │    │    └─Dropout: 5-91                     --\n",
       "│    │    │    │    └─T5GemmaCrossAttention: 5-92       --\n",
       "│    │    │    │    │    └─Linear: 6-118                589,824\n",
       "│    │    │    │    │    └─Linear: 6-119                589,824\n",
       "│    │    │    │    │    └─Linear: 6-120                589,824\n",
       "│    │    │    │    │    └─Linear: 6-121                589,824\n",
       "│    │    │    │    └─T5GemmaRMSNorm: 5-93              768\n",
       "│    │    │    │    └─T5GemmaRMSNorm: 5-94              768\n",
       "│    │    │    └─T5GemmaDecoderLayer: 4-14              --\n",
       "│    │    │    │    └─T5GemmaSelfAttention: 5-95        --\n",
       "│    │    │    │    │    └─Linear: 6-122                589,824\n",
       "│    │    │    │    │    └─Linear: 6-123                589,824\n",
       "│    │    │    │    │    └─Linear: 6-124                589,824\n",
       "│    │    │    │    │    └─Linear: 6-125                589,824\n",
       "│    │    │    │    └─T5GemmaRMSNorm: 5-96              768\n",
       "│    │    │    │    └─T5GemmaRMSNorm: 5-97              768\n",
       "│    │    │    │    └─T5GemmaMLP: 5-98                  --\n",
       "│    │    │    │    │    └─Linear: 6-126                1,572,864\n",
       "│    │    │    │    │    └─Linear: 6-127                1,572,864\n",
       "│    │    │    │    │    └─Linear: 6-128                1,572,864\n",
       "│    │    │    │    │    └─PytorchGELUTanh: 6-129       --\n",
       "│    │    │    │    │    └─Dropout: 6-130               --\n",
       "│    │    │    │    └─T5GemmaRMSNorm: 5-99              768\n",
       "│    │    │    │    └─T5GemmaRMSNorm: 5-100             768\n",
       "│    │    │    │    └─Dropout: 5-101                    --\n",
       "│    │    │    │    └─T5GemmaCrossAttention: 5-102      --\n",
       "│    │    │    │    │    └─Linear: 6-131                589,824\n",
       "│    │    │    │    │    └─Linear: 6-132                589,824\n",
       "│    │    │    │    │    └─Linear: 6-133                589,824\n",
       "│    │    │    │    │    └─Linear: 6-134                589,824\n",
       "│    │    │    │    └─T5GemmaRMSNorm: 5-103             768\n",
       "│    │    │    │    └─T5GemmaRMSNorm: 5-104             768\n",
       "│    │    │    └─T5GemmaDecoderLayer: 4-15              --\n",
       "│    │    │    │    └─T5GemmaSelfAttention: 5-105       --\n",
       "│    │    │    │    │    └─Linear: 6-135                589,824\n",
       "│    │    │    │    │    └─Linear: 6-136                589,824\n",
       "│    │    │    │    │    └─Linear: 6-137                589,824\n",
       "│    │    │    │    │    └─Linear: 6-138                589,824\n",
       "│    │    │    │    └─T5GemmaRMSNorm: 5-106             768\n",
       "│    │    │    │    └─T5GemmaRMSNorm: 5-107             768\n",
       "│    │    │    │    └─T5GemmaMLP: 5-108                 --\n",
       "│    │    │    │    │    └─Linear: 6-139                1,572,864\n",
       "│    │    │    │    │    └─Linear: 6-140                1,572,864\n",
       "│    │    │    │    │    └─Linear: 6-141                1,572,864\n",
       "│    │    │    │    │    └─PytorchGELUTanh: 6-142       --\n",
       "│    │    │    │    │    └─Dropout: 6-143               --\n",
       "│    │    │    │    └─T5GemmaRMSNorm: 5-109             768\n",
       "│    │    │    │    └─T5GemmaRMSNorm: 5-110             768\n",
       "│    │    │    │    └─Dropout: 5-111                    --\n",
       "│    │    │    │    └─T5GemmaCrossAttention: 5-112      --\n",
       "│    │    │    │    │    └─Linear: 6-144                589,824\n",
       "│    │    │    │    │    └─Linear: 6-145                589,824\n",
       "│    │    │    │    │    └─Linear: 6-146                589,824\n",
       "│    │    │    │    │    └─Linear: 6-147                589,824\n",
       "│    │    │    │    └─T5GemmaRMSNorm: 5-113             768\n",
       "│    │    │    │    └─T5GemmaRMSNorm: 5-114             768\n",
       "│    │    │    └─T5GemmaDecoderLayer: 4-16              --\n",
       "│    │    │    │    └─T5GemmaSelfAttention: 5-115       --\n",
       "│    │    │    │    │    └─Linear: 6-148                589,824\n",
       "│    │    │    │    │    └─Linear: 6-149                589,824\n",
       "│    │    │    │    │    └─Linear: 6-150                589,824\n",
       "│    │    │    │    │    └─Linear: 6-151                589,824\n",
       "│    │    │    │    └─T5GemmaRMSNorm: 5-116             768\n",
       "│    │    │    │    └─T5GemmaRMSNorm: 5-117             768\n",
       "│    │    │    │    └─T5GemmaMLP: 5-118                 --\n",
       "│    │    │    │    │    └─Linear: 6-152                1,572,864\n",
       "│    │    │    │    │    └─Linear: 6-153                1,572,864\n",
       "│    │    │    │    │    └─Linear: 6-154                1,572,864\n",
       "│    │    │    │    │    └─PytorchGELUTanh: 6-155       --\n",
       "│    │    │    │    │    └─Dropout: 6-156               --\n",
       "│    │    │    │    └─T5GemmaRMSNorm: 5-119             768\n",
       "│    │    │    │    └─T5GemmaRMSNorm: 5-120             768\n",
       "│    │    │    │    └─Dropout: 5-121                    --\n",
       "│    │    │    │    └─T5GemmaCrossAttention: 5-122      --\n",
       "│    │    │    │    │    └─Linear: 6-157                589,824\n",
       "│    │    │    │    │    └─Linear: 6-158                589,824\n",
       "│    │    │    │    │    └─Linear: 6-159                589,824\n",
       "│    │    │    │    │    └─Linear: 6-160                589,824\n",
       "│    │    │    │    └─T5GemmaRMSNorm: 5-123             768\n",
       "│    │    │    │    └─T5GemmaRMSNorm: 5-124             768\n",
       "│    │    │    └─T5GemmaDecoderLayer: 4-17              --\n",
       "│    │    │    │    └─T5GemmaSelfAttention: 5-125       --\n",
       "│    │    │    │    │    └─Linear: 6-161                589,824\n",
       "│    │    │    │    │    └─Linear: 6-162                589,824\n",
       "│    │    │    │    │    └─Linear: 6-163                589,824\n",
       "│    │    │    │    │    └─Linear: 6-164                589,824\n",
       "│    │    │    │    └─T5GemmaRMSNorm: 5-126             768\n",
       "│    │    │    │    └─T5GemmaRMSNorm: 5-127             768\n",
       "│    │    │    │    └─T5GemmaMLP: 5-128                 --\n",
       "│    │    │    │    │    └─Linear: 6-165                1,572,864\n",
       "│    │    │    │    │    └─Linear: 6-166                1,572,864\n",
       "│    │    │    │    │    └─Linear: 6-167                1,572,864\n",
       "│    │    │    │    │    └─PytorchGELUTanh: 6-168       --\n",
       "│    │    │    │    │    └─Dropout: 6-169               --\n",
       "│    │    │    │    └─T5GemmaRMSNorm: 5-129             768\n",
       "│    │    │    │    └─T5GemmaRMSNorm: 5-130             768\n",
       "│    │    │    │    └─Dropout: 5-131                    --\n",
       "│    │    │    │    └─T5GemmaCrossAttention: 5-132      --\n",
       "│    │    │    │    │    └─Linear: 6-170                589,824\n",
       "│    │    │    │    │    └─Linear: 6-171                589,824\n",
       "│    │    │    │    │    └─Linear: 6-172                589,824\n",
       "│    │    │    │    │    └─Linear: 6-173                589,824\n",
       "│    │    │    │    └─T5GemmaRMSNorm: 5-133             768\n",
       "│    │    │    │    └─T5GemmaRMSNorm: 5-134             768\n",
       "│    │    │    └─T5GemmaDecoderLayer: 4-18              --\n",
       "│    │    │    │    └─T5GemmaSelfAttention: 5-135       --\n",
       "│    │    │    │    │    └─Linear: 6-174                589,824\n",
       "│    │    │    │    │    └─Linear: 6-175                589,824\n",
       "│    │    │    │    │    └─Linear: 6-176                589,824\n",
       "│    │    │    │    │    └─Linear: 6-177                589,824\n",
       "│    │    │    │    └─T5GemmaRMSNorm: 5-136             768\n",
       "│    │    │    │    └─T5GemmaRMSNorm: 5-137             768\n",
       "│    │    │    │    └─T5GemmaMLP: 5-138                 --\n",
       "│    │    │    │    │    └─Linear: 6-178                1,572,864\n",
       "│    │    │    │    │    └─Linear: 6-179                1,572,864\n",
       "│    │    │    │    │    └─Linear: 6-180                1,572,864\n",
       "│    │    │    │    │    └─PytorchGELUTanh: 6-181       --\n",
       "│    │    │    │    │    └─Dropout: 6-182               --\n",
       "│    │    │    │    └─T5GemmaRMSNorm: 5-139             768\n",
       "│    │    │    │    └─T5GemmaRMSNorm: 5-140             768\n",
       "│    │    │    │    └─Dropout: 5-141                    --\n",
       "│    │    │    │    └─T5GemmaCrossAttention: 5-142      --\n",
       "│    │    │    │    │    └─Linear: 6-183                589,824\n",
       "│    │    │    │    │    └─Linear: 6-184                589,824\n",
       "│    │    │    │    │    └─Linear: 6-185                589,824\n",
       "│    │    │    │    │    └─Linear: 6-186                589,824\n",
       "│    │    │    │    └─T5GemmaRMSNorm: 5-143             768\n",
       "│    │    │    │    └─T5GemmaRMSNorm: 5-144             768\n",
       "│    │    │    └─T5GemmaDecoderLayer: 4-19              --\n",
       "│    │    │    │    └─T5GemmaSelfAttention: 5-145       --\n",
       "│    │    │    │    │    └─Linear: 6-187                589,824\n",
       "│    │    │    │    │    └─Linear: 6-188                589,824\n",
       "│    │    │    │    │    └─Linear: 6-189                589,824\n",
       "│    │    │    │    │    └─Linear: 6-190                589,824\n",
       "│    │    │    │    └─T5GemmaRMSNorm: 5-146             768\n",
       "│    │    │    │    └─T5GemmaRMSNorm: 5-147             768\n",
       "│    │    │    │    └─T5GemmaMLP: 5-148                 --\n",
       "│    │    │    │    │    └─Linear: 6-191                1,572,864\n",
       "│    │    │    │    │    └─Linear: 6-192                1,572,864\n",
       "│    │    │    │    │    └─Linear: 6-193                1,572,864\n",
       "│    │    │    │    │    └─PytorchGELUTanh: 6-194       --\n",
       "│    │    │    │    │    └─Dropout: 6-195               --\n",
       "│    │    │    │    └─T5GemmaRMSNorm: 5-149             768\n",
       "│    │    │    │    └─T5GemmaRMSNorm: 5-150             768\n",
       "│    │    │    │    └─Dropout: 5-151                    --\n",
       "│    │    │    │    └─T5GemmaCrossAttention: 5-152      --\n",
       "│    │    │    │    │    └─Linear: 6-196                589,824\n",
       "│    │    │    │    │    └─Linear: 6-197                589,824\n",
       "│    │    │    │    │    └─Linear: 6-198                589,824\n",
       "│    │    │    │    │    └─Linear: 6-199                589,824\n",
       "│    │    │    │    └─T5GemmaRMSNorm: 5-153             768\n",
       "│    │    │    │    └─T5GemmaRMSNorm: 5-154             768\n",
       "│    │    │    └─T5GemmaDecoderLayer: 4-20              --\n",
       "│    │    │    │    └─T5GemmaSelfAttention: 5-155       --\n",
       "│    │    │    │    │    └─Linear: 6-200                589,824\n",
       "│    │    │    │    │    └─Linear: 6-201                589,824\n",
       "│    │    │    │    │    └─Linear: 6-202                589,824\n",
       "│    │    │    │    │    └─Linear: 6-203                589,824\n",
       "│    │    │    │    └─T5GemmaRMSNorm: 5-156             768\n",
       "│    │    │    │    └─T5GemmaRMSNorm: 5-157             768\n",
       "│    │    │    │    └─T5GemmaMLP: 5-158                 --\n",
       "│    │    │    │    │    └─Linear: 6-204                1,572,864\n",
       "│    │    │    │    │    └─Linear: 6-205                1,572,864\n",
       "│    │    │    │    │    └─Linear: 6-206                1,572,864\n",
       "│    │    │    │    │    └─PytorchGELUTanh: 6-207       --\n",
       "│    │    │    │    │    └─Dropout: 6-208               --\n",
       "│    │    │    │    └─T5GemmaRMSNorm: 5-159             768\n",
       "│    │    │    │    └─T5GemmaRMSNorm: 5-160             768\n",
       "│    │    │    │    └─Dropout: 5-161                    --\n",
       "│    │    │    │    └─T5GemmaCrossAttention: 5-162      --\n",
       "│    │    │    │    │    └─Linear: 6-209                589,824\n",
       "│    │    │    │    │    └─Linear: 6-210                589,824\n",
       "│    │    │    │    │    └─Linear: 6-211                589,824\n",
       "│    │    │    │    │    └─Linear: 6-212                589,824\n",
       "│    │    │    │    └─T5GemmaRMSNorm: 5-163             768\n",
       "│    │    │    │    └─T5GemmaRMSNorm: 5-164             768\n",
       "│    │    │    └─T5GemmaDecoderLayer: 4-21              --\n",
       "│    │    │    │    └─T5GemmaSelfAttention: 5-165       --\n",
       "│    │    │    │    │    └─Linear: 6-213                589,824\n",
       "│    │    │    │    │    └─Linear: 6-214                589,824\n",
       "│    │    │    │    │    └─Linear: 6-215                589,824\n",
       "│    │    │    │    │    └─Linear: 6-216                589,824\n",
       "│    │    │    │    └─T5GemmaRMSNorm: 5-166             768\n",
       "│    │    │    │    └─T5GemmaRMSNorm: 5-167             768\n",
       "│    │    │    │    └─T5GemmaMLP: 5-168                 --\n",
       "│    │    │    │    │    └─Linear: 6-217                1,572,864\n",
       "│    │    │    │    │    └─Linear: 6-218                1,572,864\n",
       "│    │    │    │    │    └─Linear: 6-219                1,572,864\n",
       "│    │    │    │    │    └─PytorchGELUTanh: 6-220       --\n",
       "│    │    │    │    │    └─Dropout: 6-221               --\n",
       "│    │    │    │    └─T5GemmaRMSNorm: 5-169             768\n",
       "│    │    │    │    └─T5GemmaRMSNorm: 5-170             768\n",
       "│    │    │    │    └─Dropout: 5-171                    --\n",
       "│    │    │    │    └─T5GemmaCrossAttention: 5-172      --\n",
       "│    │    │    │    │    └─Linear: 6-222                589,824\n",
       "│    │    │    │    │    └─Linear: 6-223                589,824\n",
       "│    │    │    │    │    └─Linear: 6-224                589,824\n",
       "│    │    │    │    │    └─Linear: 6-225                589,824\n",
       "│    │    │    │    └─T5GemmaRMSNorm: 5-173             768\n",
       "│    │    │    │    └─T5GemmaRMSNorm: 5-174             768\n",
       "│    │    │    └─T5GemmaDecoderLayer: 4-22              --\n",
       "│    │    │    │    └─T5GemmaSelfAttention: 5-175       --\n",
       "│    │    │    │    │    └─Linear: 6-226                589,824\n",
       "│    │    │    │    │    └─Linear: 6-227                589,824\n",
       "│    │    │    │    │    └─Linear: 6-228                589,824\n",
       "│    │    │    │    │    └─Linear: 6-229                589,824\n",
       "│    │    │    │    └─T5GemmaRMSNorm: 5-176             768\n",
       "│    │    │    │    └─T5GemmaRMSNorm: 5-177             768\n",
       "│    │    │    │    └─T5GemmaMLP: 5-178                 --\n",
       "│    │    │    │    │    └─Linear: 6-230                1,572,864\n",
       "│    │    │    │    │    └─Linear: 6-231                1,572,864\n",
       "│    │    │    │    │    └─Linear: 6-232                1,572,864\n",
       "│    │    │    │    │    └─PytorchGELUTanh: 6-233       --\n",
       "│    │    │    │    │    └─Dropout: 6-234               --\n",
       "│    │    │    │    └─T5GemmaRMSNorm: 5-179             768\n",
       "│    │    │    │    └─T5GemmaRMSNorm: 5-180             768\n",
       "│    │    │    │    └─Dropout: 5-181                    --\n",
       "│    │    │    │    └─T5GemmaCrossAttention: 5-182      --\n",
       "│    │    │    │    │    └─Linear: 6-235                589,824\n",
       "│    │    │    │    │    └─Linear: 6-236                589,824\n",
       "│    │    │    │    │    └─Linear: 6-237                589,824\n",
       "│    │    │    │    │    └─Linear: 6-238                589,824\n",
       "│    │    │    │    └─T5GemmaRMSNorm: 5-183             768\n",
       "│    │    │    │    └─T5GemmaRMSNorm: 5-184             768\n",
       "│    │    │    └─T5GemmaDecoderLayer: 4-23              --\n",
       "│    │    │    │    └─T5GemmaSelfAttention: 5-185       --\n",
       "│    │    │    │    │    └─Linear: 6-239                589,824\n",
       "│    │    │    │    │    └─Linear: 6-240                589,824\n",
       "│    │    │    │    │    └─Linear: 6-241                589,824\n",
       "│    │    │    │    │    └─Linear: 6-242                589,824\n",
       "│    │    │    │    └─T5GemmaRMSNorm: 5-186             768\n",
       "│    │    │    │    └─T5GemmaRMSNorm: 5-187             768\n",
       "│    │    │    │    └─T5GemmaMLP: 5-188                 --\n",
       "│    │    │    │    │    └─Linear: 6-243                1,572,864\n",
       "│    │    │    │    │    └─Linear: 6-244                1,572,864\n",
       "│    │    │    │    │    └─Linear: 6-245                1,572,864\n",
       "│    │    │    │    │    └─PytorchGELUTanh: 6-246       --\n",
       "│    │    │    │    │    └─Dropout: 6-247               --\n",
       "│    │    │    │    └─T5GemmaRMSNorm: 5-189             768\n",
       "│    │    │    │    └─T5GemmaRMSNorm: 5-190             768\n",
       "│    │    │    │    └─Dropout: 5-191                    --\n",
       "│    │    │    │    └─T5GemmaCrossAttention: 5-192      --\n",
       "│    │    │    │    │    └─Linear: 6-248                589,824\n",
       "│    │    │    │    │    └─Linear: 6-249                589,824\n",
       "│    │    │    │    │    └─Linear: 6-250                589,824\n",
       "│    │    │    │    │    └─Linear: 6-251                589,824\n",
       "│    │    │    │    └─T5GemmaRMSNorm: 5-193             768\n",
       "│    │    │    │    └─T5GemmaRMSNorm: 5-194             768\n",
       "│    │    │    └─T5GemmaDecoderLayer: 4-24              --\n",
       "│    │    │    │    └─T5GemmaSelfAttention: 5-195       --\n",
       "│    │    │    │    │    └─Linear: 6-252                589,824\n",
       "│    │    │    │    │    └─Linear: 6-253                589,824\n",
       "│    │    │    │    │    └─Linear: 6-254                589,824\n",
       "│    │    │    │    │    └─Linear: 6-255                589,824\n",
       "│    │    │    │    └─T5GemmaRMSNorm: 5-196             768\n",
       "│    │    │    │    └─T5GemmaRMSNorm: 5-197             768\n",
       "│    │    │    │    └─T5GemmaMLP: 5-198                 --\n",
       "│    │    │    │    │    └─Linear: 6-256                1,572,864\n",
       "│    │    │    │    │    └─Linear: 6-257                1,572,864\n",
       "│    │    │    │    │    └─Linear: 6-258                1,572,864\n",
       "│    │    │    │    │    └─PytorchGELUTanh: 6-259       --\n",
       "│    │    │    │    │    └─Dropout: 6-260               --\n",
       "│    │    │    │    └─T5GemmaRMSNorm: 5-199             768\n",
       "│    │    │    │    └─T5GemmaRMSNorm: 5-200             768\n",
       "│    │    │    │    └─Dropout: 5-201                    --\n",
       "│    │    │    │    └─T5GemmaCrossAttention: 5-202      --\n",
       "│    │    │    │    │    └─Linear: 6-261                589,824\n",
       "│    │    │    │    │    └─Linear: 6-262                589,824\n",
       "│    │    │    │    │    └─Linear: 6-263                589,824\n",
       "│    │    │    │    │    └─Linear: 6-264                589,824\n",
       "│    │    │    │    └─T5GemmaRMSNorm: 5-203             768\n",
       "│    │    │    │    └─T5GemmaRMSNorm: 5-204             768\n",
       "│    │    └─Dropout: 3-10                               --\n",
       "├─T5GemmaLMHead: 1-2                                    --\n",
       "│    └─Linear: 2-3                                      196,608,000\n",
       "================================================================================\n",
       "Total params: 788,098,560\n",
       "Trainable params: 788,098,560\n",
       "Non-trainable params: 0\n",
       "================================================================================"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary(model, depth=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bfb7b11",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "T5GemmaConfig {\n",
       "  \"attention_dropout\": 0.0,\n",
       "  \"bos_token_id\": 2,\n",
       "  \"classifier_dropout_rate\": 0.0,\n",
       "  \"decoder\": {\n",
       "    \"attention_bias\": false,\n",
       "    \"attention_dropout\": 0.0,\n",
       "    \"attn_logit_softcapping\": 50.0,\n",
       "    \"cross_attention_hidden_size\": 2304,\n",
       "    \"dropout_rate\": 0.0,\n",
       "    \"final_logit_softcapping\": 30.0,\n",
       "    \"head_dim\": 256,\n",
       "    \"hidden_activation\": \"gelu_pytorch_tanh\",\n",
       "    \"hidden_size\": 2304,\n",
       "    \"initializer_range\": 0.02,\n",
       "    \"intermediate_size\": 9216,\n",
       "    \"is_decoder\": true,\n",
       "    \"layer_types\": [\n",
       "      \"sliding_attention\",\n",
       "      \"full_attention\",\n",
       "      \"sliding_attention\",\n",
       "      \"full_attention\",\n",
       "      \"sliding_attention\",\n",
       "      \"full_attention\",\n",
       "      \"sliding_attention\",\n",
       "      \"full_attention\",\n",
       "      \"sliding_attention\",\n",
       "      \"full_attention\",\n",
       "      \"sliding_attention\",\n",
       "      \"full_attention\",\n",
       "      \"sliding_attention\",\n",
       "      \"full_attention\",\n",
       "      \"sliding_attention\",\n",
       "      \"full_attention\",\n",
       "      \"sliding_attention\",\n",
       "      \"full_attention\",\n",
       "      \"sliding_attention\",\n",
       "      \"full_attention\",\n",
       "      \"sliding_attention\",\n",
       "      \"full_attention\",\n",
       "      \"sliding_attention\",\n",
       "      \"full_attention\",\n",
       "      \"sliding_attention\",\n",
       "      \"full_attention\"\n",
       "    ],\n",
       "    \"max_position_embeddings\": 8192,\n",
       "    \"model_type\": \"t5_gemma_module\",\n",
       "    \"num_attention_heads\": 8,\n",
       "    \"num_hidden_layers\": 26,\n",
       "    \"num_key_value_heads\": 4,\n",
       "    \"query_pre_attn_scalar\": 256,\n",
       "    \"rms_norm_eps\": 1e-06,\n",
       "    \"rope_theta\": 10000.0,\n",
       "    \"sliding_window\": 4096,\n",
       "    \"use_cache\": true,\n",
       "    \"vocab_size\": 256000\n",
       "  },\n",
       "  \"dropout_rate\": 0.0,\n",
       "  \"encoder\": {\n",
       "    \"attention_bias\": false,\n",
       "    \"attention_dropout\": 0.0,\n",
       "    \"attn_logit_softcapping\": 50.0,\n",
       "    \"dropout_rate\": 0.0,\n",
       "    \"final_logit_softcapping\": 30.0,\n",
       "    \"head_dim\": 256,\n",
       "    \"hidden_activation\": \"gelu_pytorch_tanh\",\n",
       "    \"hidden_size\": 2304,\n",
       "    \"initializer_range\": 0.02,\n",
       "    \"intermediate_size\": 9216,\n",
       "    \"layer_types\": [\n",
       "      \"sliding_attention\",\n",
       "      \"full_attention\",\n",
       "      \"sliding_attention\",\n",
       "      \"full_attention\",\n",
       "      \"sliding_attention\",\n",
       "      \"full_attention\",\n",
       "      \"sliding_attention\",\n",
       "      \"full_attention\",\n",
       "      \"sliding_attention\",\n",
       "      \"full_attention\",\n",
       "      \"sliding_attention\",\n",
       "      \"full_attention\",\n",
       "      \"sliding_attention\",\n",
       "      \"full_attention\",\n",
       "      \"sliding_attention\",\n",
       "      \"full_attention\",\n",
       "      \"sliding_attention\",\n",
       "      \"full_attention\",\n",
       "      \"sliding_attention\",\n",
       "      \"full_attention\",\n",
       "      \"sliding_attention\",\n",
       "      \"full_attention\",\n",
       "      \"sliding_attention\",\n",
       "      \"full_attention\",\n",
       "      \"sliding_attention\",\n",
       "      \"full_attention\"\n",
       "    ],\n",
       "    \"max_position_embeddings\": 8192,\n",
       "    \"model_type\": \"t5_gemma_module\",\n",
       "    \"num_attention_heads\": 8,\n",
       "    \"num_hidden_layers\": 26,\n",
       "    \"num_key_value_heads\": 4,\n",
       "    \"query_pre_attn_scalar\": 256,\n",
       "    \"rms_norm_eps\": 1e-06,\n",
       "    \"rope_theta\": 10000.0,\n",
       "    \"sliding_window\": 4096,\n",
       "    \"use_cache\": true,\n",
       "    \"vocab_size\": 256000\n",
       "  },\n",
       "  \"eos_token_id\": 1,\n",
       "  \"initializer_range\": 0.02,\n",
       "  \"is_encoder_decoder\": true,\n",
       "  \"model_type\": \"t5gemma\",\n",
       "  \"pad_token_id\": 0,\n",
       "  \"transformers_version\": \"4.56.0\",\n",
       "  \"use_cache\": true,\n",
       "  \"vocab_size\": 256000\n",
       "}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config = T5GemmaConfig(\n",
    "    intermediate_size=120,\n",
    "    num_hidden_layers=2,\n",
    ")\n",
    "config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f162d8f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 2,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"pad_token_id\": 0\n",
      "}\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mT5GemmaForConditionalGeneration\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mT5GemmaConfig\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/CloudStorage/GoogleDrive-matteobonotto90@gmail.com/My Drive/Colab_Notebooks/triton/triton/venv/lib/python3.10/site-packages/transformers/models/t5gemma/modeling_t5gemma.py:1014\u001b[0m, in \u001b[0;36mT5GemmaForConditionalGeneration.__init__\u001b[0;34m(self, config)\u001b[0m\n\u001b[1;32m   1011\u001b[0m config\u001b[38;5;241m.\u001b[39mis_encoder_decoder \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m   1012\u001b[0m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(config)\n\u001b[0;32m-> 1014\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel \u001b[38;5;241m=\u001b[39m \u001b[43mT5GemmaModel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1015\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvocab_size \u001b[38;5;241m=\u001b[39m config\u001b[38;5;241m.\u001b[39mdecoder\u001b[38;5;241m.\u001b[39mvocab_size\n\u001b[1;32m   1016\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlm_head \u001b[38;5;241m=\u001b[39m T5GemmaLMHead(config\u001b[38;5;241m.\u001b[39mdecoder\u001b[38;5;241m.\u001b[39mhidden_size, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvocab_size)\n",
      "File \u001b[0;32m~/Library/CloudStorage/GoogleDrive-matteobonotto90@gmail.com/My Drive/Colab_Notebooks/triton/triton/venv/lib/python3.10/site-packages/transformers/models/t5gemma/modeling_t5gemma.py:893\u001b[0m, in \u001b[0;36mT5GemmaModel.__init__\u001b[0;34m(self, config)\u001b[0m\n\u001b[1;32m    890\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m config\u001b[38;5;241m.\u001b[39mis_encoder_decoder:\n\u001b[1;32m    891\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mT5GemmaModel only support encoder-decoder modeling. Use `T5GemmaEncoderModel` instead.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 893\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencoder \u001b[38;5;241m=\u001b[39m \u001b[43mT5GemmaEncoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoder\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    894\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdecoder \u001b[38;5;241m=\u001b[39m T5GemmaDecoder(config\u001b[38;5;241m.\u001b[39mdecoder)\n\u001b[1;32m    896\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpost_init()\n",
      "File \u001b[0;32m~/Library/CloudStorage/GoogleDrive-matteobonotto90@gmail.com/My Drive/Colab_Notebooks/triton/triton/venv/lib/python3.10/site-packages/transformers/models/t5gemma/modeling_t5gemma.py:705\u001b[0m, in \u001b[0;36mT5GemmaEncoder.__init__\u001b[0;34m(self, config)\u001b[0m\n\u001b[1;32m    702\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mDropout(config\u001b[38;5;241m.\u001b[39mdropout_rate)\n\u001b[1;32m    704\u001b[0m \u001b[38;5;66;03m# Initialize weights and apply final processing\u001b[39;00m\n\u001b[0;32m--> 705\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpost_init\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/CloudStorage/GoogleDrive-matteobonotto90@gmail.com/My Drive/Colab_Notebooks/triton/triton/venv/lib/python3.10/site-packages/transformers/modeling_utils.py:2232\u001b[0m, in \u001b[0;36mPreTrainedModel.post_init\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   2224\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mpost_init\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m   2225\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   2226\u001b[0m \u001b[38;5;124;03m    A method executed at the end of each Transformer model initialization, to execute code that needs the model's\u001b[39;00m\n\u001b[1;32m   2227\u001b[0m \u001b[38;5;124;03m    modules properly initialized (such as weight initialization).\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2230\u001b[0m \u001b[38;5;124;03m    the model's tp_plan!\u001b[39;00m\n\u001b[1;32m   2231\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 2232\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minit_weights\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2233\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_compatibility_gradient_checkpointing()\n\u001b[1;32m   2235\u001b[0m     \u001b[38;5;66;03m# Make sure the modules correctly exist if the flag is active\u001b[39;00m\n",
      "File \u001b[0;32m~/Library/CloudStorage/GoogleDrive-matteobonotto90@gmail.com/My Drive/Colab_Notebooks/triton/triton/venv/lib/python3.10/site-packages/transformers/modeling_utils.py:3769\u001b[0m, in \u001b[0;36mPreTrainedModel.init_weights\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   3765\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprune_heads(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mpruned_heads)\n\u001b[1;32m   3767\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _init_weights:\n\u001b[1;32m   3768\u001b[0m     \u001b[38;5;66;03m# Initialize weights\u001b[39;00m\n\u001b[0;32m-> 3769\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minitialize_weights\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3771\u001b[0m     \u001b[38;5;66;03m# Tie weights should be skipped when not initializing all weights\u001b[39;00m\n\u001b[1;32m   3772\u001b[0m     \u001b[38;5;66;03m# since from_pretrained(...) calls tie weights anyways\u001b[39;00m\n\u001b[1;32m   3773\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtie_weights()\n",
      "File \u001b[0;32m~/Library/CloudStorage/GoogleDrive-matteobonotto90@gmail.com/My Drive/Colab_Notebooks/triton/triton/venv/lib/python3.10/site-packages/torch/utils/_contextlib.py:120\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    117\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    118\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    119\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 120\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/CloudStorage/GoogleDrive-matteobonotto90@gmail.com/My Drive/Colab_Notebooks/triton/triton/venv/lib/python3.10/site-packages/transformers/modeling_utils.py:3102\u001b[0m, in \u001b[0;36mPreTrainedModel.initialize_weights\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   3099\u001b[0m     torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mModule\u001b[38;5;241m.\u001b[39msmart_apply \u001b[38;5;241m=\u001b[39m smart_apply\n\u001b[1;32m   3101\u001b[0m \u001b[38;5;66;03m# Let the magic happen with this simple call\u001b[39;00m\n\u001b[0;32m-> 3102\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msmart_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_initialize_weights\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/CloudStorage/GoogleDrive-matteobonotto90@gmail.com/My Drive/Colab_Notebooks/triton/triton/venv/lib/python3.10/site-packages/transformers/modeling_utils.py:3095\u001b[0m, in \u001b[0;36mPreTrainedModel.initialize_weights.<locals>.smart_apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m   3093\u001b[0m         module\u001b[38;5;241m.\u001b[39msmart_apply(module\u001b[38;5;241m.\u001b[39m_initialize_weights)\n\u001b[1;32m   3094\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 3095\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msmart_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3096\u001b[0m fn(\u001b[38;5;28mself\u001b[39m)\n\u001b[1;32m   3097\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[0;32m~/Library/CloudStorage/GoogleDrive-matteobonotto90@gmail.com/My Drive/Colab_Notebooks/triton/triton/venv/lib/python3.10/site-packages/transformers/modeling_utils.py:3095\u001b[0m, in \u001b[0;36mPreTrainedModel.initialize_weights.<locals>.smart_apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m   3093\u001b[0m         module\u001b[38;5;241m.\u001b[39msmart_apply(module\u001b[38;5;241m.\u001b[39m_initialize_weights)\n\u001b[1;32m   3094\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 3095\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msmart_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3096\u001b[0m fn(\u001b[38;5;28mself\u001b[39m)\n\u001b[1;32m   3097\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "    \u001b[0;31m[... skipping similar frames: PreTrainedModel.initialize_weights.<locals>.smart_apply at line 3095 (1 times)]\u001b[0m\n",
      "File \u001b[0;32m~/Library/CloudStorage/GoogleDrive-matteobonotto90@gmail.com/My Drive/Colab_Notebooks/triton/triton/venv/lib/python3.10/site-packages/transformers/modeling_utils.py:3095\u001b[0m, in \u001b[0;36mPreTrainedModel.initialize_weights.<locals>.smart_apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m   3093\u001b[0m         module\u001b[38;5;241m.\u001b[39msmart_apply(module\u001b[38;5;241m.\u001b[39m_initialize_weights)\n\u001b[1;32m   3094\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 3095\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msmart_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3096\u001b[0m fn(\u001b[38;5;28mself\u001b[39m)\n\u001b[1;32m   3097\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[0;32m~/Library/CloudStorage/GoogleDrive-matteobonotto90@gmail.com/My Drive/Colab_Notebooks/triton/triton/venv/lib/python3.10/site-packages/transformers/modeling_utils.py:3096\u001b[0m, in \u001b[0;36mPreTrainedModel.initialize_weights.<locals>.smart_apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m   3094\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   3095\u001b[0m         module\u001b[38;5;241m.\u001b[39msmart_apply(fn)\n\u001b[0;32m-> 3096\u001b[0m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3097\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[0;32m~/Library/CloudStorage/GoogleDrive-matteobonotto90@gmail.com/My Drive/Colab_Notebooks/triton/triton/venv/lib/python3.10/site-packages/transformers/modeling_utils.py:3070\u001b[0m, in \u001b[0;36mPreTrainedModel._initialize_weights\u001b[0;34m(self, module)\u001b[0m\n\u001b[1;32m   3068\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(module, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_is_hf_initialized\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m   3069\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[0;32m-> 3070\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_init_weights\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodule\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3071\u001b[0m module\u001b[38;5;241m.\u001b[39m_is_hf_initialized \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m~/Library/CloudStorage/GoogleDrive-matteobonotto90@gmail.com/My Drive/Colab_Notebooks/triton/triton/venv/lib/python3.10/site-packages/transformers/models/t5gemma/modeling_t5gemma.py:603\u001b[0m, in \u001b[0;36mT5GemmaPreTrainedModel._init_weights\u001b[0;34m(self, module)\u001b[0m\n\u001b[1;32m    601\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_init_weights\u001b[39m(\u001b[38;5;28mself\u001b[39m, module):\n\u001b[1;32m    602\u001b[0m     \u001b[38;5;66;03m# TODO: support intialization for encoders and decoders separately(?)\u001b[39;00m\n\u001b[0;32m--> 603\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_init_weights\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodule\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    604\u001b[0m     std \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39minitializer_range\n\u001b[1;32m    605\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(module, T5GemmaClassificationHead):\n",
      "File \u001b[0;32m~/Library/CloudStorage/GoogleDrive-matteobonotto90@gmail.com/My Drive/Colab_Notebooks/triton/triton/venv/lib/python3.10/site-packages/transformers/modeling_utils.py:3041\u001b[0m, in \u001b[0;36mPreTrainedModel._init_weights\u001b[0;34m(self, module)\u001b[0m\n\u001b[1;32m   3038\u001b[0m     std \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mget_text_config(), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minitializer_range\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m0.02\u001b[39m)\n\u001b[1;32m   3040\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(module, (nn\u001b[38;5;241m.\u001b[39mLinear, nn\u001b[38;5;241m.\u001b[39mConv1d, nn\u001b[38;5;241m.\u001b[39mConv2d, nn\u001b[38;5;241m.\u001b[39mConv3d, nn\u001b[38;5;241m.\u001b[39mConvTranspose1d, nn\u001b[38;5;241m.\u001b[39mConvTranspose2d)):\n\u001b[0;32m-> 3041\u001b[0m     \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnormal_\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmean\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstd\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstd\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3042\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m module\u001b[38;5;241m.\u001b[39mbias \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   3043\u001b[0m         module\u001b[38;5;241m.\u001b[39mbias\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mzero_()\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# model = T5GemmaForConditionalGeneration(config=T5GemmaConfig())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe36df5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
